\documentclass[draftcls]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mathrsfs}
\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\usepackage[colorlinks=true,pagebackref,linkcolor=magenta]{hyperref}
\usepackage{cite}
%\usepackage[colon,sort&compress]{natbib}
%\numberwithin{equation}{section}
\renewcommand\arraystretch{1.2}
\let\underbrace\LaTeXunderbrace
\let\overbrace\LaTeXoverbrace
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\bibliographystyle{IEEEtran}
\begin{document}
\title{Attribute fusion in a latent process model for time series of
  graphs}
\author{Carey~E.~Priebe, Nam~H.~Lee, Youngser~Park, and Minh~Tang}%
%\thanks{Johns Hopkins University \\ Department of Applied Mathematics
%  and Statistics \\ Baltimore, Maryland 21218-2682 USA}%
\maketitle
\begin{abstract}
 We consider the problem of anomaly/change point detection given a
 time series of graphs with categorical attributes on the
 edges. Various attributed graph invariants are considered, and their
 power for detection as a function of a linear fusion parameter is
 presented.  
\end{abstract}
\begin{IEEEkeywords}
  Anomaly detection, Attributed Random Graphs, Fusion, Random Dot
  Product Graphs
\end{IEEEkeywords}
\section{Introduction}
\subsection{Latent Process Model}
The latent process model for time series of attributed graphs was
originally presented in
\cite{lee:_laten_proces_model_time_attrib_random_graph}. We summarized
here the ideas that are relevant to our discussion. We begin by
introducing some terminology. Let $\mathscr{S}$ be the unit simplex in
$\mathbb{R}^{K}$, i.e.,
\begin{equation}
  \mathscr{S} = \{ \xi \in [0,1]^{K}
  \colon \sum_{k = 1}^{K} \xi_k \leq 1 \}.
\end{equation}
A {\em random dot product space} for attributed graphs with vertices
in $[n]$ and edge attributes $\mathscr{K} = [K]
$ is then a pair $(\mathbf{X},G)$ of random elements such that
\begin{enumerate}
\item $\mathbf{X} = \{X_i\}_{i = 1}^{n}$ is a collection of
  $\mathscr{S}$-valued random vectors.
\item $G$ is a random graph with vertices set $[n]$ such that
  \begin{equation}
    \label{eq:1}
    \mathbb{P}(i \sim j \,|\, \mathbf{X} = (x_1, x_2, \dots,
    x_n)) = \langle x_i, x_j \rangle.
  \end{equation}
  and that $\mathbf{P}(i \sim j \,|\, \mathbf{X})$ and $\mathbf{P}(i' \sim
  j' \,|\, \mathbf{X})$ are independent whenever $(i,j) \not = (i',j')$. If
  $i \sim j$ in $G$, then the attribute of the edge $\{i,j\}$ is an
  element of $\mathscr{K}$. In particular, $\{i,j\}$ has attribute $k$
  with probability $x_{i,k} x_{j,k}$. 
\end{enumerate}
Let $\mathscr{K}_{+} = \{1,\dots,K+1\}$. We say that a c\'{a}dl\'{a}g
process $W \colon [0,\infty) \mapsto \mathscr{K}_{+}^{n}$ induces the
sequence of random dot product spaces $\mathscr{V} = \{X(t), G(t)\}_{t
  = 1}^{\infty}$ if
\begin{enumerate}
\item Each $(X(t), G(t))$ is a random dot product space with vertices
  $[n]$ and attributes $\mathscr{K}$. Furthermore, for each
  $i \in [n]$, $k \in \mathscr{K}$ and $t \in \mathbb{N}$, we have
  $X_{i,k}(t)  = \int_{t - 1}^{t}{ \mathbf{1}\{W_i(u) = k\}\, du}$.
\item  For each $t \in \mathbb{N}$, we have
  \begin{equation}
    \label{eq:2}
    \mathbb{P}(G(t) = g \,|\, \mathscr{F}_{\leq t}) = \mathbb{P}(G(t) = g \,|\, X(t))
  \end{equation}
where $\mathscr{F}_{\leq t}$ are the sigma fields generated by $\{W(s)
  \colon s \leq t\}$.
\end{enumerate}
We will call any pair $(\mathscr{V}, W)$ that satisfies the above
properties a random dot process model.  In particular, we are
interested in the pairs $(\mathscr{V}, W)$ possessing the following
properties:
\begin{enumerate}
\item For each $t \in \mathbb{N}$ and vertex $i \in [n]$, there exists
  a matrix $\mathbf{Q}^{(i)}(t)$ such that $W_i$, when restricted to
  the interval $[t, t+1)$, is a stationary, continuous-time Markov
  chain with state space $\mathscr{K}_+$, intensity matrix
  ${\mathbf{Q}^{(i)}(t)}$, and stationary distribution
  $\pi^{(i)}(t)$. $W$ is thus a stationary, continuous-time Markov chain with state
  space $\mathscr{K}_{+}^{n}$ and intentisy matrix
  $\otimes_{i=1}^{n}\mathbf{Q}^{(i)}(t)$.
\item There exists a $t^{*} \in \mathbb{N}$ and a $m < n$ such that 
  \begin{enumerate}
  \item for  $t < t^{*}$
    \begin{gather*}
      \pi^{(i)}(t) \equiv \pi_0 \\
      Q^{(i)}(t) \equiv \mathbf{Q}_0
    \end{gather*}
  \item  for $t \geq t^{*}$
    \begin{gather*}
      \pi^{(1)}(t) = \dots = \pi^{(m)}(t) = \pi_1 \\
      \pi^{(m+1)}(t)  = \dots = \pi^{(n)}(t) = \pi_0 \\
      \mathbf{Q}^{(1)}(t)  = \dots = \mathbf{Q}^{(m)}(t) = \mathbf{Q}_1 \\
      \mathbf{Q}^{(m+1)}(t) = \dots = \mathbf{Q}^{(n)}(t) = \mathbf{Q}_0 
    \end{gather*}
  \end{enumerate}
\end{enumerate}
The above properties characterize a random dot process model with a
change-point phenomena. We will refer to $(t^{*}, m, \pi_0, \pi_1,
\mathbf{Q}_0, \mathbf{Q}_1)$ as the change parameters. Because $W$ is completely
determined by the change parameters, we often choose to omit $W$ and
only mention $\mathscr{V}$ when referring to random dot process models
with change-point phenomena. For a rdpm $\mathscr{V}$ with change
parameters $(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$, we can construct
several approximations to $\mathscr{V}$. Of particular interests are the
following two approximations.
\begin{definition}
  \label{def:1}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The first order approximation
  $\bar{\mathscr{V}}$ of $\mathscr{V}$ is the sequence $\{(\bar{X}(t),
  \bar{G}(t)\}_{t = 1}^{\infty}$ of independent random dot product
  spaces such that
 \begin{enumerate}
 \item For $t < t^{*}$,
   \begin{equation}
     \label{eq:5}
     \bar{X}_{i}(t) \equiv \bar{\pi}_0.
   \end{equation}
 \item For $t \geq t^{*}$
   \begin{gather*}
     \bar{X}_{i}(t) \equiv \bar{\pi}_1 \quad \text{for $i \leq m$} \\
     \bar{X}_{i}(t) \equiv \bar{\pi}_0 \quad \text{for $i > m$} 
   \end{gather*}
 \end{enumerate}
 where $\bar{\pi}_0$ and $\bar{\pi}_1$ are sub-probability vectors
 obtained by removing the last coordinate of $\pi_0$ and $\pi_1$. 
\end{definition}
\begin{definition}
  \label{def:2}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, Q_0, Q_1)$. Define $\mathbf{Z}_0$ and $\mathbf{Z}_1$
  by
  \begin{gather}
    \mathbf{Z}_0 = (\mathbf{1}\mathbf{\pi}_0^{T} -
    \mathbf{Q}_0)^{-1}(\mathbf{I} - \mathbf{1}\pi_0^{T}) \\
    \mathbf{Z}_1 = (\mathbf{1}\mathbf{\pi}_1^{T} -
    \mathbf{Q}_1)^{-1}(\mathbf{I} - \mathbf{1}\pi_1^{T})
  \end{gather}
  $\mathbf{Z}_0$ and $\mathbf{Z}_1$ are the fundamental matrices
  for the continuous-time Markov chain on $\mathscr{K}$ with intensity
  matrix $\mathbf{Q}_0$ and $\mathbf{Q}_1$ (see
  e.g., \cite[p. 55]{asmussen03:_applied_probab_queues}). Let
  $\Sigma_0$ and $\Sigma_1$ be given by
  \begin{gather*}
    \Sigma_0 = \mathrm{diag}(\pi_0) \mathbf{Z}_0 + \mathbf{Z}_0^{T}
    \mathrm{diag}(\pi_0) \\
    \Sigma_1 = \mathrm{diag}(\pi_1) \mathbf{Z}_1 + \mathbf{Z}_1^{T}
    \mathrm{diag}(\pi_1)
  \end{gather*}
  A second order approximation $\hat{\mathscr{V}}$ of $\mathscr{V}$
  is the sequence $\{\hat{X}(t), \hat{G}(t)\}_{t=1}^{\infty}$ where
  \begin{enumerate}
  \item For each $t$ and each $i \in [n]$, $\hat{X}^{i}(t)$ is a
    random vector obtained by truncating a multivariate normal random
    vector $Z_{i}(t)$ to $\mathscr{S}$.
  \item For $t < t^{*}$,
    \begin{gather}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0 \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma^{0}}
    \end{gather}
  \item For $t \geq t^{*}$,  
    \begin{gather*}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0, \quad \text{for $i \leq
        m$}  \\
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_1, \quad \text{for $i > 
        m$}  \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{0} \quad \text{for $i
        \leq m$} \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{1} \quad \text{for $i
        > m$}
    \end{gather*}
    where $\hat{\Sigma}_0$ and $\hat{\Sigma}_1$ are the matrices
    obtained by removing last row and column of $\Sigma_0$ and
    $\Sigma_1$. 
  \end{enumerate}
\end{definition}
\section{Change-point detection}
Let $\mathscr{V}$ be a random dot process model with change parameters
$(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The
change-parameters encapsulate a notion of chatter anomalies, i.e., a
subset of vertices of $\mathscr{V}$ with altered communication
behaviour in an otherwise stationary setting as depicted in Fig.~\ref{fig:notional_change_point}. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig1-SSP2011.pdf}
  \caption{Notional depiction of the problem of change-point detection in a time-series of graphs}
  \label{fig:notional_change_point}
\end{figure}
\\ \noindent
We are interested in
the problem of testing, for a $t \in \mathbb{N}$, the hypotheses that
$t$ is the change-point of $\mathscr{V}$, namely
\begin{gather*}
  \mathscr{H}_0 \colon t^{*} > t \\
  \mathscr{H}_A \colon t^{*} = t
\end{gather*}
%\subsection{Graphs Invariants}
%In this paper we consider the problem of detecting chatter anomalies
This will be done using the notion of fusion of attributed graph
invariants. The particular invariants of interests are the size
$\mathcal{E}$, number of triangles $\tau_{\lambda}$, scan $\Psi$, and
max degree $\Delta$. Specifically, we consider linear attribute
fusion with parameter $\lambda \in \mathbb{R}^{K}$ via
\begin{gather}
  \label{eq:6}
  \mathcal{E}_{\lambda}(G(t)) = \sum_{k=1}^{K} \lambda_k \sum_{u,v}
  \mathbb{I}\{ \phi(uv,t) = k \} \\
  \tau_\lambda(G(t)) = \sum_{k=1}^{K} \lambda_k \!\! \sum_{(u,v,w) \in
    \binom{V}{3}} h_k(u,v,w) \\
  \Delta_{\lambda}(G(t)) = \max_{v \in V} \, \sum_{k = 1}^{K} \lambda_k
  \sum_{u \in N(v)}{\mathbb{I}\{\phi(uv,t) = k\}} \\
  \label{eq:3}
  \Psi_{\lambda}(G(t)) = \max_{v \in V} \sum_{k = 1}^{K}
  \lambda_k \!\!\sum_{u,w \in N(v)}\!\!\! \mathbb{I}\{\phi(uw,t) = k\} 
  \end{gather}
where $h_k(u,v,w)$ is the indicator function that the argument, a set
of three vertices, forms a triangle of type $k$. A triangle
is of type $k$ provided that all edges in the triangle have attribute
$k$. \\ \\
\noindent
Let $J_{\lambda}(t)$ be a statistic of the form as in Eq.~\eqref{eq:6} through
Eq.~\eqref{eq:3}. We define the normalization $\bar{J}^{l}_\lambda(t)$
of $J_{\lambda}$ based on recent past as
\begin{equation}
  \label{eq:4}
 \bar{J}^{l}_{\lambda}(t) = \frac{1}{l}\sum_{s = 1}^{l} J_{\lambda}(t - s) 
\end{equation}
where $l \in \mathbb{N}$ specified the width of the running-average
window. Our main interests is in the normalized fusion
statistic $T_{\lambda}^{l}(t)$ as depicted in Fig.~\ref{fig:temporal}, namely
\begin{equation}
  \label{eq:7}
 T_{\lambda}^{l}(t) = % \begin{cases}
   % J_{\lambda}(t) - J_{\lambda}(t - 1) & \text{if $m = 1$} \\
   \frac{J_{\lambda}(t) -
     \bar{J}_{\lambda}^{l}(t)}{\sqrt{\tfrac{1}{l-1}
       \sum_{s=1}^{l}(J_{\lambda}(t - s) - \bar{J}_{\lambda}^{l}(t))^2}}
%   & \text{if $m \geq 2$}
 %  \end{cases}
\end{equation}
for $l \geq 2$. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig4-SSP2011.pdf}
  \caption{Temporal standardization: when testing for change at time
    $t$, the recent past (graphs $G(t - l), \dots, G(t-1))$ is used to
    standardize the invariants}
  \label{fig:temporal}
\end{figure}
\subsection{Power estimates for $\mathcal{E}_\lambda(t)$}
It was shown in
\cite{lee:_laten_proces_model_time_attrib_random_graph} that for the
case where $J_\lambda(t)$ is $\mathcal{E}_{\lambda}(t)$, $T_{\lambda}^{l}(t)$ 
follows a $t$-distribution with $l - 1$
degrees of freedom in the limit. Specifically, we have the following
results
\begin{lemma}
  \label{lem:1}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. Define the vector $\zeta$ and the matrix $\xi$ by
  \begin{gather*}
    \zeta = \tbinom{m}{2}(\pi_{11} - \pi_{00}) + (n-m)m(\pi_{01} -
    \pi_{00}) \\
    \xi = \tfrac{l+1}{l}\tbinom{n}{2} \eta_{00} +
          \tbinom{m}{2}(\eta_{11} - \eta_{00}) + (n-m)m(\eta_{01} -
          \eta_{00})
  \end{gather*}
  Define the random variable $\psi^{l}_{\lambda}(t)$ by
  \begin{equation}
    \label{eq:10}
    \psi^{l}_{\lambda}(t) = \begin{cases}
      \sqrt{\frac{\langle \lambda, \tbinom{n}{2} \eta_{00}
            \lambda \rangle}{\langle \lambda, \xi
            \lambda \rangle}} T_{\lambda}^{l}(t) & \text{if $t = t^{*}$} \\
      \sqrt{\frac{l}{l + 1}} T_{\lambda}^{l}(t)& \text{if $t < t^{*}$}
      \end{cases}
  \end{equation}
As $n \rightarrow \infty$,
  $\psi^{l}_{\lambda}(t)$ converges weakly to the Student
  $t$-distribution with $l-1$ degrees of freedom and non-centrality
  parameter $\mu_{\lambda}$, where
  \begin{equation}
    \label{eq:15}
    \mu_{\lambda} = \begin{cases}
      \frac{\langle \lambda, \zeta \rangle}{\sqrt{\langle \lambda, \xi \lambda \rangle}} & \text{if $t = t^{*}$} \\
      0 & \text{if $t < t^{*}$}
    \end{cases}
  \end{equation}
\end{lemma}
We are interested in finding the $\lambda$ that will maximize the
power of the test. The power approximation is determined by various
factors but the dominating factor is likely to be
$\mu_{\lambda}$. Theorem \ref{thm:1} extends a result in
\cite{lee:_laten_proces_model_time_attrib_random_graph} for $K = 2$ to the case
where $K \geq 2$. 
\begin{theorem}
  \label{thm:1}
  Let $\zeta$ and $\xi$ be as defined in Lemma \ref{lem:1}.  Suppose
  that $\xi$ is also positive definite. Let $\nu^{*}$ be the
  normalized eigenvector corresponding to the largest eigenvalue of $ \xi^{-1/2}
  \zeta \zeta^{T} \xi^{-1/2}$, i.e.,
 \begin{equation}
   \label{eq:9}
  \nu^{*} = \argmax_{\nu \colon \| \nu \| = 1}
  \nu^{T} \xi^{-1/2} \zeta \zeta^{T} \xi^{-1/2}
  \nu.
 \end{equation}
 Then $\lambda^{*} = \tfrac{\xi^{-1/2} \nu^{*}}{\|\xi^{-1/2} \nu^{*} \|}$ satisfies
 \begin{gather}
   \label{eq:8}
 \argmax_{ \|\lambda\| = 1}
 \mu_{\lambda} \, \cap \, \{\lambda^{*}, - \lambda^{*}\} \not = \emptyset \\
 \argmin_{ \| \lambda \| = 1} \mu_{\lambda} \, \cap \, \{\lambda^{*}, -
 \lambda^{*}\} \not = \emptyset.
 \end{gather}
\end{theorem}
\subsection{Power estimates for $\Delta_{\lambda}(t)$}
The analysis of the power estimates for $\Delta_{\lambda}(t)$ depends
on some preliminary results which we now state. We denote by
$\mathcal{G}(\alpha, \beta)$ the Gumbel distribution with location
parameter $\alpha$ and scale parameter $\beta$.  
\begin{proposition}
  \label{prop:3}
  Let $a_n$ and $b_n$ be functions of $n$ given by
  \begin{align*}
    a_n &= (2 \log{n})^{1/2}\Bigl(1 - \frac{\log{\log{n}}}{4 \log{n}} -
    \frac{\log{2\pi^{1/2}}}{2 \log{n}} \Bigr) \\ 
    b_n &= (2 \log{n})^{-1/2}
  \end{align*}
  Then as $n \rightarrow \infty$ and $m = \Omega( \sqrt{n \log{n}})$,
  we have
  \begin{equation}
    \label{eq:14}
    \Delta_{\lambda}(t) \rightsquigarrow \begin{cases}
      \mathcal{G}(a_n \sigma_1 + \mu_1, b_n \sigma_1 ) & \text{for $t < t^{*}$.} \\
     \mathcal{G}(a_m \sigma_2 + \mu_2, b_m \sigma_2) &
     \text{for $t = t^{*}$.}
     \end{cases}
  \end{equation}
  where
  \begin{align*}
    \mu_1 &= (n-1)\langle \lambda, \pi_{00} \rangle \\
    \sigma_1 &= \sqrt{(n-1)\langle \lambda, \eta_{00} \lambda \rangle} \\
    \mu_2 &= \,\, (m - 1) \langle x, \pi_{11} \rangle + (n-
    m)\langle \lambda, \pi_{01} \rangle
    \\ \sigma_2 &= \sqrt{ (m - 1) \langle \lambda, \eta_{11} \lambda \rangle + (n -
      m) \langle \lambda, \eta_{01} \lambda \rangle}
    \end{align*}
\end{proposition}
\begin{lemma}
  \label{lem:2}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. For sufficiently large $n$ and sufficiently large
  $l$, the variable $T_{\lambda}^{l}(t)$ has approximately a
  $\mathcal{G}(\rho_{\lambda}, \varsigma_{\lambda})$ distribution with
\begin{align}
  \label{eq:13}
  \rho_{\lambda} &= \begin{cases}
    \frac{\sqrt{\pi}}{6} \frac{\mu_2 - \mu_1 + a_m\sigma_2 - (a_n + b_n
      \gamma)\sigma_1}{b_n \sigma_1} & \text{if $t =
        t^{*}$} \\
      - \frac{ \sqrt{\pi}}{6} \gamma & \text{if $t < t^{*}$}
  \end{cases}\\
  \varsigma_{\lambda} &= \begin{cases}
    \frac{\sqrt{\pi} b_m \sigma_2}{6 b_n \sigma_1} & \text{if $t =
      t^{*}$} \\
    \frac{\sqrt{\pi}}{6} & \text{if $t < t^{*}$}
    \end{cases}
\end{align}
$\gamma \approx 0.57721$ is the Euler-Mascheroni constant.
\end{lemma}
We are interested in finding the $\lambda$ that will maximize the
power of the test using $\Delta_{\lambda}$. The power approximation is
once again determined by various factors, but the dominating factor is
likely to be $\tfrac{\rho_\lambda}{\varsigma_\lambda}$. For sufficiently large $n$
and $m$, we also have
\begin{equation*}
  \begin{split}
  \frac{\rho_\lambda}{\varsigma_\lambda} &= (1 + o(1)) \frac{\mu_2 - \mu_1}{b_m
    \sigma_ 2} \\ &= \frac{\langle \lambda, (m-1)\pi_{11} + (n-m)\pi_{01} -
    (n-1) \pi_{00} \rangle}{\sqrt{\langle \lambda, ((m-1) \eta_{11} +
    (n-m)\eta_{01}) \lambda \rangle}}
  \end{split}
\end{equation*}
We can thus find the maximum and minimum of
$\tfrac{\rho_{\lambda}}{\varsigma_{\lambda}}$ by solving an 
eigenvalue problem as in Theorem~\ref{thm:1}. 
\subsection{Power estimates for $\Psi_{\lambda}(t)$}
The limiting distribution for the scan statistics for unattributed
random graphs was considered in
\cite{rukhin:_limit_distr_graph_scan_statis}. This subsection is
concerned with adpating the results in
\cite{rukhin:_limit_distr_graph_scan_statis} to attributed random
graphs. We state here only the relevant results. The proofs are
delegated to the appendices. \\ \\
\noindent
The limiting distribution for $\Psi_{\lambda}(t)$ is Gumbel for both
the case where $t < t^{*}$ and for $t = t^{*}$. To aid the exposition,
we first introduce some notations. Let $\lambda^{(2)}$ be the
element-wise square of $\lambda$. Define $C_{00}$, $C_{01}$, $C_{11}$
and $p_{00}$, $p_{01}$, $p_{11}$ to be
\begin{gather}
  \label{eq:19}
  C_{00} = \tfrac{\langle \lambda^{(2)}, \pi_{00} \rangle}{\langle \lambda,
    \pi_{00}\rangle}; \quad p_{00} = \tfrac{(\langle \lambda, \pi_{00}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{00} \rangle} \\
C_{11} = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{11}\rangle}; \quad p_{11} = \tfrac{(\langle \lambda, \pi_{11}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{11} \rangle} \\
C_{10} = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{10}\rangle}; \quad p_{10} = \tfrac{(\langle \lambda, \pi_{10}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{10} \rangle} 
\end{gather}
Also let $z_n$ be defined, for any $n \in \mathbb{N}$, by
\begin{equation}
  \label{eq:20}
   z_n = \sqrt{2
      \log{n}}\Bigl(1 - \frac{\log{\log{n}} + \log{4 \pi}}{4
      \log{n}}\Bigr)
\end{equation}
\begin{theorem}
  \label{thm:2}
  Let $\Psi_{\lambda}(t)$ be the scan statistic for $t < t^{*}$. Let
  $N_{0}$, $a_n$, and $b_n$ be defined by
\begin{align*}
   \mu_0 &= (n-1) \langle 1, \pi_{00} \rangle \\
   \sigma_0 &= \sqrt{(n-1) (\langle 1, \pi_{00} \rangle - \langle
     1, \pi_{00} \rangle^2)} \\ 
    N_{0} &= \mu_0 + z_n \sigma_0 \\
    a_n &= N_{0} \frac{\langle \lambda, \pi_{00} \rangle}{\langle
      1, \pi_{00} \rangle} + C_{00}p_{00}\tbinom{N_{0}}{2} \\ 
   b_n &= (1 - \tfrac{C_{00}p_{00}}{2} + C_{00}p_{00} N_{0}) \frac{\sigma_0}{\sqrt{2 \log
     n}}
\end{align*}
Then we have
\begin{equation}
  \frac{\Psi_{\lambda}(t) - a_{n}}{b_n} \rightsquigarrow
  \mathcal{G}(0, 1)
\end{equation}
\end{theorem}%
We now move to the limiting distribution for $\Psi_{\lambda}(t^{*})$. 
Let us define $E$ and $F$ by
\begin{gather*}
 %  B \sim \mathrm{Bin}(m, \langle 1, \pi_{00} \rangle); \quad C \sim \mathrm{Bin}(n-m-1,
 % \langle 1, \pi_{01} \rangle) \\
  E \sim \mathrm{Bin}(m-1, \langle \pi_{11}, 1 \rangle) \\ F \sim \mathrm{Bin}(n-m,
  \langle \pi_{01}, 1 \rangle)
\end{gather*}%
For any random variable $X$, we let $\mu_X$ and $\sigma_X^{2}$ denote
the mean and variance of $X$. If $X$ and $Y$ are independent random
variables, we let $\mu_{X + Y}$ and $\sigma_{X + Y}^2$ denote the mean
and variance of the convolution $X + Y$. \\
\noindent Let $N_{\kappa}$ be the quantity defined by
\begin{equation}
  \label{eq:18}
  N_{\kappa} = \mu_{E + F} + z_m \sigma_{E + F}
\end{equation}
\begin{theorem}
  \label{thm:3}
  Let $a_{n,m}$ and $b_{n,m}$ be given by
  \begin{gather}
    \label{eq:28}
    \begin{split}
    a_{n,m} &=%  N_{\kappa} \frac{\langle x, \pi_{01} \rangle}{\langle
 %      1, \pi_{01} \rangle} \\
 % & +
    C_{00} p_{00} \Bigl(\tbinom{N_\kappa}{2} - \tbinom{\mu_E}{2} - \mu_E
    \mu_F\Bigr) \\ 
     & + C_{11} p_{11} \tbinom{\mu_E}{2} + C_{01} p_{01} \mu_E \mu_F 
    \end{split} \\
    b_{n,m} = (1 - \tfrac{C_{00} p_{00}}{2} + C_{00} p_{00}
    N_\kappa)\frac{\sigma_{E + F}}{\sqrt{2 \log{n_1}}}
  \end{gather}
  If $m = \Omega(\sqrt{n \log n})$, then
  \begin{equation}
    \label{eq:29}
    \frac{\Psi_{\lambda}(t^{*}) - a_{n,m}}{b_{n,m}}  \rightsquigarrow \mathcal{G}(0,1)  \end{equation}
\end{theorem}
\appendices
\section{Proofs of some stated results}
\label{sec:proofs-some-stated}
\begin{proof}[Theorem \ref{thm:1}]
  The maximizer of $\mu_\lambda$ also maximizes
  \begin{equation*}
    \mu_{\lambda}^{2} = \frac{\lambda^{T} \zeta \zeta^{T}
      \lambda}{\lambda^{T} \xi \lambda}
  \end{equation*}
  Because $\xi$ is positive definite, there exist a positive definite matrix
  $\xi^{1/2}$ such that $\xi^{1/2} \xi^{1/2} = \xi$. Letting $\nu = \xi^{1/2}
  \lambda$, the above expression can be rewritten as
  \begin{equation*}
    \mu_{\lambda}^{2} = \frac{\nu^{T} \xi^{-1/2} \zeta \zeta^{T}
      \xi^{-1/2} \nu}{ \nu^{T} \nu}
  \end{equation*}
  The claim then follows directly from the Rayleigh-Ritz theorem for
  Hermitean matrices.
\end{proof}
\begin{proof}[Proposition~\ref{prop:3}]
  Let $d_{\lambda}(v;t)$ be the (fused) degree of vertex $v$ at time
  $t$, i.e.,
  \begin{equation*}
    d_{\lambda}(v;t) = \sum_{k = 1}^{K} \lambda_k \sum_{w \in N(v)}
    \mathbb{I}\{ \phi(vw,t) = k \}
  \end{equation*}
  Let $\Gamma^{(k)}(v;t) = |\{w \colon \mathbb{I}\{\phi(wv,t) =
  k\}|$. The vector $\bm{\Gamma}(v;t) = (\Gamma^{(1)}(v;t), \dots,
  \Gamma^{(K)}(v;t))$ for $t < t^{*}$ is distributed as a multinomial
  with $n-1$ trials and probability vector $\pi_0$. By the multivariate
  normal approximation to the multinomial, we have
  \begin{equation*}
    \bm{\Gamma}(v;t) \approx \mathcal{N}((n-1)\pi_0,
    (n-1)(\mathrm{diag}(\pi_0) - \pi_0 \pi_0^{T}))
  \end{equation*}
  Now, $d_{\lambda}(v;t) = \langle \lambda, \bm{\Gamma}(v;t) \rangle$
  and thus, for sufficiently large $n$ we have
  \begin{equation*}
    d_{\lambda}(v;t) = (1 + o_P(1)) \mathcal{N}((n-1) \langle \lambda, \pi_0
    \rangle, (n-1) \langle \lambda, \eta_{00} \lambda \rangle)
  \end{equation*}
  where $\eta_{00} = \mathrm{diag}(\pi_0) - \pi_0 \pi_0^{T}$ and
  $o_P(1)$ denotes a random variables that converges to $0$ in
  probability. We can thus take $\Delta_{\lambda}(t)$ for $t < t^{*}$
  as the maximum of a sequence of {\em dependent} Gaussian random
  variables. By an argument analogous to the argument for the maximum
  degree of an Erd\"{o}s-Renyi random graph in \cite[\S
  III.1]{bollobas85:_random_graph} we can show that the dependency
  among the $d_{\lambda}(v,t)$ can be safely
  ignored. $\Delta_{\lambda}(t)$ can thus be taken as the maximum of
  {\em independent} $\mathcal{N}((n-1) \langle \lambda,
  \pi_0 \rangle, (n-1) \langle \lambda, \eta_{00} \lambda
  \rangle)$. The resulting distribution is a Gumbel \cite[\S
  2.3]{galambos87:_asymp_theor_extrem_order_statis} and we obtain
  Eq.~\eqref{eq:14} for $t < t^{*}$. The case when $t = t^{*}$ can be
  derive in a similar manner, see e.g,
  \cite{rukhin:_limit_distr_graph_scan_statis}.
\end{proof}
\begin{proof}[Lemma~\ref{lem:2}]
  Let $ X \sim \mathcal{G}(\alpha, \beta)$. We consider the
  normalization $\tfrac{X - \mu}{\sigma}$. We have
  \begin{equation*}
    \begin{split}
    \mathbb{P}[ \frac{X - \mu}{\sigma} \leq z]  &= \mathbb{P}[X \leq z
    \sigma + \mu] 
    = e^{-e^{-(z \sigma + \mu - \alpha)/\beta}} \\
      &= e^{- e^{-(z - (\alpha - \mu)/\sigma)/(\beta/\sigma)}}
    \end{split}
  \end{equation*}
  Thus, $\tfrac{X - \mu}{\sigma} \sim \mathcal{G}(\tfrac{\alpha -
    \mu}{\sigma}, \tfrac{\beta}{\sigma})$. Because the sample
  mean and the sample variance are consistent estimators, the claims
  follow after an application of Slutsky's theorem.
\end{proof}
Suppose that $\eta_{00}$ is positive definite. We then have the
following result
\begin{proposition}
  \label{prop:1}
  Let $C = \tfrac{\langle \lambda^{(2)}, \pi_{00} \rangle}{\langle
    \lambda, \pi_{00}\rangle}$ and $p = \tfrac{(\langle \lambda, \pi_{00}
    \rangle)^{2}}{\langle x^{(2)} \pi_{00} \rangle}$. Then $p < 1$,
  and for sufficiently large $l$ we have
\end{proposition}
Proposition~\ref{prop:1} states that the locality
statistics for the attributed random graphs model under consideration
can be approximated by the locality statistics for an
Erd\"{o}s-Renyi graph with edge probability $p$. By a result in
\cite{rukhin:_limit_distr_graph_scan_statis}, we have
We now move to the case where $t = t^{*}$. Let $d_{\zeta}(v;t)$ be
the number of neighbors of $v$ that lies in $[m]$ (the egg) and
$d_{\xi}(v;t)$ be the number of neighbors of $v$ that lies in $[n]
\setminus [m]$ (the kidney). Conditional on $d_{\zeta}(v;t^{*}) =
l_{\zeta}$, $d_{\xi}(v;t^{*}) = l_{\xi}$, we have
\begin{equation}
  \psi_{\lambda}(v;t^{*}) = \sum_{k=1}^{K} \lambda_k ( y^{(\zeta)}_k + y^{(\xi)}_k +
  y^{(\omega)}_k + z^{(\zeta)}_k + z^{(\xi)}_k)
\end{equation}
where $(y^{(\zeta)}_1, \dots, y^{(\zeta)}_m)$, $(y^{(\xi)}_1,\dots,
 y^{(\xi)}_m)$, $(y^{(\omega)}_1, \dots, y^{(\omega)}_m)$ are
 distributed as
\begin{gather*}
(y^{(\zeta)}_1,\dots,y^{(\zeta)}_m) \sim \textrm{multinomial}\Bigl(
\tbinom{l_\zeta}{2}, \pi_{11}\Bigr) \\ 
(y^{(\xi)}_1,\dots,y^{(\xi)}_m) \sim \textrm{multinomial}\Bigl(
\tbinom{l_\xi}{2}, \pi_{00}\Bigr) \\
(y^{(\omega)}_1,\dots,y^{(\omega)}_m) \sim \textrm{multinomial}\Bigl(
l_\zeta l_\xi, \pi_{10}\Bigr)
\end{gather*}
and $(z^{(\zeta)}_1, \dots, z^{(\zeta)}_m)$, $(z^{(\xi)}_1, \dots,
z^{(\xi)}_m)$ are distributed as
\begin{gather*}
(z^{(\zeta)}_1, \dots, z^{(\zeta)}_m) \sim
\begin{cases}
\textrm{multinomial}\Bigl(l_\zeta, \frac{\pi_{11}}{\langle \pi_{11},
  1\rangle}\Bigr) & \text{if $v \in [m]$} \\
\textrm{multinomial}\Bigl(l_\zeta, \frac{\pi_{10}}{\langle \pi_{10}, 1
  \rangle} \Bigr) & \text{if $v \not\in [m]$}
\end{cases} \\
(z^{(\xi)}_1, \dots, z^{(\xi)}_m) \sim
\begin{cases}
\textrm{multinomial}\Bigl(l_\xi, \frac{\pi_{10}}{\langle \pi_{10},
  1\rangle}\Bigr) & \text{if $v \in [m]$} \\
\textrm{multinomial}\Bigl(l_\zeta, \frac{\pi_{00}}{\langle \pi_{00}, 1
  \rangle} \Bigr) & \text{if $v \notin [m]$}
\end{cases} 
\end{gather*}
Thus, for sufficiently large $l_{\zeta}$ and $l_{\xi}$, we have
\begin{equation}
  \begin{split}
 \psi_{\lambda}(v;t^{*}) & \sim  \mathcal{N}\Bigl(\tbinom{l_\zeta}{2}
 \langle \lambda, \pi_{11} \rangle,
  \tbinom{l_\zeta}{2} \langle x, \eta_{11} \lambda \rangle \Bigr) \\ &+ 
  \mathcal{N}\Bigl(\tbinom{l_\xi}{2} \langle \lambda, \pi_{00} \rangle,
  \tbinom{l_\xi}{2} \langle \lambda, \eta_{00} \lambda \rangle \Bigr) \\ &+ 
  \mathcal{N}\Bigl(l_\zeta l_\xi \langle \lambda, \pi_{10} \rangle,
  l_\zeta l_\xi \langle \lambda, \eta_{10} \lambda \rangle \Bigr)
\end{split}
\end{equation}
\begin{proposition}
  \label{prop:2}
  Let $C_1, C_2, C_3$ and $p_1$, $p_2$, $p_3$ be 
  \begin{gather*}
C_1 = \tfrac{\langle \lambda^{(2)}, \pi_{00} \rangle}{\langle \lambda,
    \pi_{00}\rangle}; \quad p_1 = \tfrac{(\langle \lambda, \pi_{00}
    \rangle)^{2}}{\langle \lambda^{(2)} \pi_{00} \rangle} \\
C_2 = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{11}\rangle}; \quad p_2 = \tfrac{(\langle \lambda, \pi_{11}
    \rangle)^{2}}{\langle \lambda^{(2)} \pi_{11} \rangle} \\
C_3 = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{10}\rangle}; \quad p_1 = \tfrac{(\langle \lambda, \pi_{10}
    \rangle)^{2}}{\langle \lambda^{(2)} \pi_{10} \rangle} 
\end{gather*}
  Then $p_1$, $p_2$, and $p_3$ are all at most $1$. Furthermore, for
  sufficiently large $l_\zeta$ and $l_\xi$ we have
\begin{equation*}
  \begin{split}
 \psi_{\lambda}(v;t_1) &\sim C_1 *  
 \mathrm{Bin}\Bigl(\tbinom{l_\zeta}{2}, p_1\Bigr) + C_2 *  
 \mathrm{Bin}\Bigl(\tbinom{l_\xi}{2}, p_2\Bigr) \\ &+ C_3 *
 \mathrm{Bin}\Bigl(l_\zeta l_\xi, p_3\Bigr)
 \end{split}
\end{equation*} 
\end{proposition}
Proposition~\ref{prop:2} state that the locality statistics
for the attributed random graphs model under consideration at time $t
= t^{*}$ can be approximated by the locality
statistics for an unattributed kidney and egg random graph model
\begin{proof}[Theorem~\ref{thm:2}]
Let $\psi_{\lambda}(v;t)$ be the (fused) locality statistics for
vertex $v$ at time $t$, i.e.,
\begin{equation}
  \label{eq:11}
  \psi_{\lambda}(v;t) = \sum_{k=1}^{K} \lambda_k \sum_{uw \in N(v)}
  \mathbb{I}\{\phi(uw,t) = k\}.
\end{equation}
The following statements are made conditional on $|N(v)| = l$. First
of all, we have
\begin{equation*}
  \psi_{\lambda}(v;t) = \sum_{k=1}^{K}{\lambda_k (y_k + z_k)}
\end{equation*}
where $(y_1, \dots, y_K)$ and $(z_1, \dots, z_K)$ are distributed as
\begin{gather*}
  (y_1,y_2,\dots,y_K) \sim \textrm{multinomial}\Bigl(
  \tbinom{l}{2}, \pi_{00}\Bigr) \\ (z_1, z_2, \dots, z_K) \sim
  \textrm{multinomial}\Bigl(l, \frac{\pi_{00}}{\langle \pi_{00},
    1\rangle}\Bigr). 
\end{gather*}
For large $l$, by the multivariate normal
approximation to the multinomial we have
\begin{equation*}
  \begin{split}
  \psi_\lambda(v;t) & = (1 + o_{P}(1)) \mathcal{N}\Bigl(\tbinom{l}{2} \langle \lambda, \pi_{00} \rangle,
  \tbinom{l}{2} \langle \lambda, \eta_{00} \lambda \rangle \Bigr) \\ 
  &= (1 + o_{P}(1)) C_{00} \mathcal{N}(\tbinom{l}{2} p_{00}, \tbinom{l}{2} p_{00}
  (1 - p_{00}))
  \end{split}
\end{equation*} 
Now $p_{00} \leq 1$ with strict inequality whenever $\eta_{00}$ is
positive definite. We thus have, for sufficiently large $l$,
\begin{equation}
  \label{eq:17}
 \psi_\lambda(v;t) = (1 + o_P(1)) C_{00} *  
 \mathrm{Bin}\Bigl(\tbinom{l}{2}, p_{00}\Bigr)
\end{equation} 
Eq.~\eqref{eq:17} states that the locality statistics for the
attributed random graphs model under considation with $t < t^{*}$ can
be approximated by the locality statistics for an Erd\"{o}s-Renyi
graph with edge probability $p_{00}$. The claim then follows from
\cite[Theorem 1.1]{rukhin:_limit_distr_graph_scan_statis}.
\end{proof}
\bibliography{ssp2011}
\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
