\documentclass[draftcls]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mathrsfs}
\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\usepackage[colorlinks=true,pagebackref,linkcolor=magenta]{hyperref}
\usepackage{cite}
%\usepackage[colon,sort&compress]{natbib}
%\numberwithin{equation}{section}
\renewcommand\arraystretch{1.2}
\let\underbrace\LaTeXunderbrace
\let\overbrace\LaTeXoverbrace
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\bibliographystyle{IEEEtran}
\begin{document}
\title{Attribute fusion in a latent process model for time series of
  graphs}
\author{Carey~E.~Priebe, Nam~H.~Lee, Youngser~Park, and Minh~Tang}%
%\thanks{Johns Hopkins University \\ Department of Applied Mathematics
%  and Statistics \\ Baltimore, Maryland 21218-2682 USA}%
\maketitle
\begin{abstract}
 We consider the problem of anomaly/change point detection given a
 time series of graphs with categorical attributes on the
 edges. Various attributed graph invariants are considered, and their
 power for detection as a function of a linear fusion parameter is
 presented.  
\end{abstract}
\begin{IEEEkeywords}
  Anomaly detection, Attributed Random Graphs, Fusion, Random Dot
  Product Graphs
\end{IEEEkeywords}
\section{Introduction}
\subsection{Latent Process Model}
The latent process model for time series of attributed graphs was
originally presented in
\cite{lee:_laten_proces_model_time_attrib_random_graph}. We summarized
here the ideas that are relevant to our discussion. We begin by
introducing some terminology. Let $\mathscr{S}$ be the unit simplex in
$\mathbb{R}^{K}$, i.e.,
\begin{equation}
  \mathscr{S} = \{ \xi \in [0,1]^{K}
  \colon \sum_{k = 1}^{K} \xi_k \leq 1 \}.
\end{equation}
A {\em random dot product space} for attributed graphs with vertices
in $[n]$ and edge attributes $\mathscr{K} = [K]
$ is then a pair $(\mathbf{X},G)$ of random elements such that
\begin{enumerate}
\item $\mathbf{X} = \{X_i\}_{i = 1}^{n}$ is a collection of
  $\mathscr{S}$-valued random vectors.
\item $G$ is a random graph with vertices set $[n]$ such that
  \begin{equation}
    \label{eq:1}
    \mathbb{P}(i \sim j \,|\, \mathbf{X} = (x_1, x_2, \dots,
    x_n)) = \langle x_i, x_j \rangle.
  \end{equation}
  and that $\mathbf{P}(i \sim j \,|\, \mathbf{X})$ and $\mathbf{P}(i' \sim
  j' \,|\, \mathbf{X})$ are independent whenever $(i,j) \not = (i',j')$. If
  $i \sim j$ in $G$, then the attribute of the edge $\{i,j\}$ is an
  element of $\mathscr{K}$. In particular, $\{i,j\}$ has attribute $k$
  with probability $x_{i,k} x_{j,k}$. 
\end{enumerate}
Let $\mathscr{K}_{+} = \{1,\dots,K+1\}$. We say that a c\'{a}dl\'{a}g
process $W \colon [0,\infty) \mapsto \mathscr{K}_{+}^{n}$ induces the
sequence of random dot product spaces $\mathscr{V} = \{X(t), G(t)\}_{t
  = 1}^{\infty}$ if
\begin{enumerate}
\item Each $(X(t), G(t))$ is a random dot product space with vertices
  $[n]$ and attributes $\mathscr{K}$. Furthermore, for each
  $i \in [n]$, $k \in \mathscr{K}$ and $t \in \mathbb{N}$, we have
  $X_{i,k}(t)  = \int_{t - 1}^{t}{ \mathbf{1}\{W_i(u) = k\}\, du}$.
\item  For each $t \in \mathbb{N}$, we have
  \begin{equation}
    \label{eq:2}
    \mathbb{P}(G(t) = g \,|\, \mathscr{F}_{\leq t}) = \mathbb{P}(G(t) = g \,|\, X(t))
  \end{equation}
where $\mathscr{F}_{\leq t}$ are the sigma fields generated by $\{W(s)
  \colon s \leq t\}$.
\end{enumerate}
We will call any pair $(\mathscr{V}, W)$ that satisfies the above
properties a random dot process model.  In particular, we are
interested in the pairs $(\mathscr{V}, W)$ possessing the following
properties:
\begin{enumerate}
\item For each $t \in \mathbb{N}$ and vertex $i \in [n]$, there exists
  a matrix $\mathbf{Q}^{(i)}(t)$ such that $W_i$, when restricted to
  the interval $[t, t+1)$, is a stationary, continuous-time Markov
  chain with state space $\mathscr{K}_+$, intensity matrix
  ${\mathbf{Q}^{(i)}(t)}$, and stationary distribution
  $\pi^{(i)}(t)$. $W$ is thus a stationary, continuous-time Markov chain with state
  space $\mathscr{K}_{+}^{n}$ and intentisy matrix
  $\otimes_{i=1}^{n}\mathbf{Q}^{(i)}(t)$.
\item There exists a $t^{*} \in \mathbb{N}$ and a $m < n$ such that 
  \begin{enumerate}
  \item for  $t < t^{*}$
    \begin{gather*}
      \pi^{(i)}(t) \equiv \pi_0 \\
      Q^{(i)}(t) \equiv \mathbf{Q}_0
    \end{gather*}
  \item  for $t \geq t^{*}$
    \begin{gather*}
      \pi^{(1)}(t) = \dots = \pi^{(m)}(t) = \pi_1 \\
      \pi^{(m+1)}(t)  = \dots = \pi^{(n)}(t) = \pi_0 \\
      \mathbf{Q}^{(1)}(t)  = \dots = \mathbf{Q}^{(m)}(t) = \mathbf{Q}_1 \\
      \mathbf{Q}^{(m+1)}(t) = \dots = \mathbf{Q}^{(n)}(t) = \mathbf{Q}_0 
    \end{gather*}
  \end{enumerate}
\end{enumerate}
The above properties characterize a random dot process model with a
change-point phenomena. We will refer to $(t^{*}, m, \pi_0, \pi_1,
\mathbf{Q}_0, \mathbf{Q}_1)$ as the change parameters. Because $W$ is completely
determined by the change parameters, we often choose to omit $W$ and
only mention $\mathscr{V}$ when referring to random dot process models
with change-point phenomena. For a rdpm $\mathscr{V}$ with change
parameters $(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$, we can construct
several approximations to $\mathscr{V}$. Of particular interests are the
following two approximations.
\begin{definition}
  \label{def:1}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The first order approximation
  $\bar{\mathscr{V}}$ of $\mathscr{V}$ is the sequence $\{(\bar{X}(t),
  \bar{G}(t)\}_{t = 1}^{\infty}$ of independent random dot product
  spaces such that
 \begin{enumerate}
 \item For $t < t^{*}$,
   \begin{equation}
     \label{eq:5}
     \bar{X}_{i}(t) \equiv \bar{\pi}_0.
   \end{equation}
 \item For $t \geq t^{*}$
   \begin{gather*}
     \bar{X}_{i}(t) \equiv \bar{\pi}_1 \quad \text{for $i \leq m$} \\
     \bar{X}_{i}(t) \equiv \bar{\pi}_0 \quad \text{for $i > m$} 
   \end{gather*}
 \end{enumerate}
 where $\bar{\pi}_0$ and $\bar{\pi}_1$ are sub-probability vectors
 obtained by removing the last coordinate of $\pi_0$ and $\pi_1$. 
\end{definition}
\begin{definition}
  \label{def:2}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, Q_0, Q_1)$. Define $\mathbf{Z}_0$ and $\mathbf{Z}_1$
  by
  \begin{gather}
    \mathbf{Z}_0 = (\mathbf{1}\mathbf{\pi}_0^{T} -
    \mathbf{Q}_0)^{-1}(\mathbf{I} - \mathbf{1}\pi_0^{T}) \\
    \mathbf{Z}_1 = (\mathbf{1}\mathbf{\pi}_1^{T} -
    \mathbf{Q}_1)^{-1}(\mathbf{I} - \mathbf{1}\pi_1^{T})
  \end{gather}
  $\mathbf{Z}_0$ and $\mathbf{Z}_1$ are the fundamental matrices
  for the continuous-time Markov chain on $\mathscr{K}$ with intensity
  matrix $\mathbf{Q}_0$ and $\mathbf{Q}_1$ (see
  e.g., \cite[p. 55]{asmussen03:_applied_probab_queues}). Let
  $\Sigma_0$ and $\Sigma_1$ be given by
  \begin{gather*}
    \Sigma_0 = \mathrm{diag}(\pi_0) \mathbf{Z}_0 + \mathbf{Z}_0^{T}
    \mathrm{diag}(\pi_0) \\
    \Sigma_1 = \mathrm{diag}(\pi_1) \mathbf{Z}_1 + \mathbf{Z}_1^{T}
    \mathrm{diag}(\pi_1)
  \end{gather*}
  A second order approximation $\hat{\mathscr{V}}$ of $\mathscr{V}$
  is the sequence $\{\hat{X}(t), \hat{G}(t)\}_{t=1}^{\infty}$ where
  \begin{enumerate}
  \item For each $t$ and each $i \in [n]$, $\hat{X}^{i}(t)$ is a
    random vector obtained by truncating a multivariate normal random
    vector $Z_{i}(t)$ to $\mathscr{S}$.
  \item For $t < t^{*}$,
    \begin{gather}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0 \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma^{0}}
    \end{gather}
  \item For $t \geq t^{*}$,  
    \begin{gather*}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0, \quad \text{for $i \leq
        m$}  \\
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_1, \quad \text{for $i > 
        m$}  \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{0} \quad \text{for $i
        \leq m$} \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{1} \quad \text{for $i
        > m$}
    \end{gather*}
    where $\hat{\Sigma}_0$ and $\hat{\Sigma}_1$ are the matrices
    obtained by removing last row and column of $\Sigma_0$ and
    $\Sigma_1$. 
  \end{enumerate}
\end{definition}
\section{Change-point detection}
Let $\mathscr{V}$ be a random dot process model with change parameters
$(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The
change-parameters encapsulate a notion of chatter anomalies, i.e., a
subset of vertices of $\mathscr{V}$ with altered communication
behaviour in an otherwise stationary setting as depicted in Fig.~\ref{fig:notional_change_point}. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig1-SSP2011.pdf}
  \caption{Notional depiction of the problem of change-point detection in a time-series of graphs}
  \label{fig:notional_change_point}
\end{figure}
\\ \noindent
We are interested in
the problem of testing, for a $t \in \mathbb{N}$, the hypotheses that
$t$ is the change-point of $\mathscr{V}$, namely
\begin{gather*}
  \mathscr{H}_0 \colon t^{*} > t \\
  \mathscr{H}_A \colon t^{*} = t
\end{gather*}
%\subsection{Graphs Invariants}
%In this paper we consider the problem of detecting chatter anomalies
This will be done using the notion of fusion of attributed graph
invariants. The particular invariants of interests are the size
$\mathcal{E}$, number of triangles $\tau_{\lambda}$, scan $\Psi$, and
max degree $\Delta$. Specifically, we consider linear attribute
fusion with parameter $\lambda \in \mathbb{R}^{K}$ via
\begin{gather}
  \label{eq:6}
  \mathcal{E}_{\lambda}(G(t)) = \sum_{k=1}^{K} \lambda_k \sum_{u,v}
  \mathbb{I}\{ \phi(uv,t) = k \} \\
  \tau_\lambda(G(t)) = \sum_{k=1}^{K} \lambda_k \!\! \sum_{(u,v,w) \in
    \binom{V}{3}} h_k(u,v,w) \\
  \Delta_{\lambda}(G(t)) = \max_{v \in V} \, \sum_{k = 1}^{K} \lambda_k
  \sum_{u \in N(v)}{\mathbb{I}\{\phi(uv,t) = k\}} \\
  \label{eq:3}
  \Psi_{\lambda}(G(t)) = \max_{v \in V} \sum_{k = 1}^{K}
  \lambda_k \!\!\sum_{u,w \in N(v)}\!\!\! \mathbb{I}\{\phi(uw,t) = k\} 
  \end{gather}
where $h_k(u,v,w)$ is the indicator function that the argument, a set
of three vertices, forms a triangle of type $k$. A triangle
is of type $k$ provided that all edges in the triangle have attribute
$k$. \\ \\
\noindent
Let $J_{\lambda}(t)$ be a statistic of the form as in Eq.~\eqref{eq:6} through
Eq.~\eqref{eq:3}. We define the normalization $\bar{J}^{l}_\lambda(t)$
of $J_{\lambda}$ based on recent past as
\begin{equation}
  \label{eq:4}
 \bar{J}^{l}_{\lambda}(t) = \frac{1}{l}\sum_{s = 1}^{l} J_{\lambda}(t - s) 
\end{equation}
where $l \in \mathbb{N}$ specified the width of the running-average
window. Our main interests is in the normalized fusion
statistic $T_{\lambda}^{l}(t)$ as depicted in Fig.~\ref{fig:temporal}, namely
\begin{equation}
  \label{eq:7}
 T_{\lambda}^{l}(t) = % \begin{cases}
   % J_{\lambda}(t) - J_{\lambda}(t - 1) & \text{if $m = 1$} \\
   \frac{J_{\lambda}(t) -
     \bar{J}_{\lambda}^{l}(t)}{\sqrt{\tfrac{1}{l-1}
       \sum_{s=1}^{l}(J_{\lambda}(t - s) - \bar{J}_{\lambda}^{l}(t))^2}}
%   & \text{if $m \geq 2$}
 %  \end{cases}
\end{equation}
for $l \geq 2$. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig4-SSP2011.pdf}
  \caption{Temporal standardization: when testing for change at time
    $t$, the recent past (graphs $G(t - l), \dots, G(t-1))$ is used to
    standardize the invariants}
  \label{fig:temporal}
\end{figure}
\subsection{Power estimates for $\mathcal{E}_\lambda(t)$}
It was shown in
\cite{lee:_laten_proces_model_time_attrib_random_graph} that for the
case where $J_\lambda(t)$ is $\mathcal{E}_{\lambda}(t)$, $T_{\lambda}^{l}(t)$ 
follows a $t$-distribution with $l - 1$
degrees of freedom in the limit. Specifically, we have the following
results
\begin{lemma}
  \label{lem:1}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. Define the vector $\zeta$ and the matrix $\xi$ by
  \begin{gather*}
    \zeta = \tbinom{m}{2}(\pi_{11} - \pi_{00}) + (n-m)m(\pi_{01} -
    \pi_{00}) \\
    \xi = \tfrac{l+1}{l}\tbinom{n}{2} \eta_{00} +
          \tbinom{m}{2}(\eta_{11} - \eta_{00}) + (n-m)m(\eta_{01} -
          \eta_{00})
  \end{gather*}
  Define the random variable $\psi^{l}_{\lambda}(t)$ by
  \begin{equation}
    \label{eq:10}
    \psi^{l}_{\lambda}(t) = \begin{cases}
      \sqrt{\frac{\langle \lambda, \tbinom{n}{2} \eta_{00}
            \lambda \rangle}{\langle \lambda, \xi
            \lambda \rangle}} T_{\lambda}^{l}(t) & \text{if $t = t^{*}$} \\
      \sqrt{\frac{l}{l + 1}} T_{\lambda}^{l}(t)& \text{if $t < t^{*}$}
      \end{cases}
  \end{equation}
As $n \rightarrow \infty$,
  $\psi^{l}_{\lambda}(t)$ converges weakly to the Student
  $t$-distribution with $l-1$ degrees of freedom and non-centrality
  parameter $\mu_{\lambda}$, where
  \begin{equation}
    \label{eq:15}
    \mu_{\lambda} = \begin{cases}
      \frac{\langle \lambda, \zeta \rangle}{\sqrt{\langle \lambda, \xi \lambda \rangle}} & \text{if $t = t^{*}$} \\
      0 & \text{if $t < t^{*}$}
    \end{cases}
  \end{equation}
\end{lemma}
We are interested in finding the $\lambda$ that will maximize the
power of the test. The power approximation is determined by various
factors but the dominating factor is likely to be
$\mu_{\lambda}$. Theorem \ref{thm:1} extends a result in
\cite{lee:_laten_proces_model_time_attrib_random_graph} for $K = 2$ to the case
where $K \geq 2$. 
\begin{theorem}
  \label{thm:1}
  Let $\zeta$ and $\xi$ be as defined in Lemma \ref{lem:1}.  Suppose
  that $\xi$ is also positive definite. Let $\nu^{*}$ be the
  normalized eigenvector corresponding to the largest eigenvalue of $ \xi^{-1/2}
  \zeta \zeta^{T} \xi^{-1/2}$, i.e.,
 \begin{equation}
   \label{eq:9}
  \nu^{*} = \argmax_{\nu \colon \| \nu \| = 1}
  \nu^{T} \xi^{-1/2} \zeta \zeta^{T} \xi^{-1/2}
  \nu.
 \end{equation}
 Then $\lambda^{*} = \tfrac{\xi^{-1/2} \nu^{*}}{\|\xi^{-1/2} \nu^{*} \|}$ satisfies
 \begin{gather}
   \label{eq:8}
 \argmax_{ \|\lambda\| = 1}
 \mu_{\lambda} \, \cap \, \{\lambda^{*}, - \lambda^{*}\} \not = \emptyset \\
 \argmin_{ \| \lambda \| = 1} \mu_{\lambda} \, \cap \, \{\lambda^{*}, -
 \lambda^{*}\} \not = \emptyset.
 \end{gather}
\end{theorem}
\subsection{Power estimates for $\Delta_{\lambda}(t)$}
The analysis of the power estimates for $\Delta_{\lambda}(t)$ depends
on some preliminary results which we now state. We denote by
$\mathcal{G}(\alpha, \beta)$ the Gumbel distribution with location
parameter $\alpha$ and scale parameter $\beta$.  
\begin{proposition}
  \label{prop:3}
  Let $a_n$ and $b_n$ be functions of $n$ given by
  \begin{align*}
    a_n &= (2 \log{n})^{1/2}\Bigl(1 - \frac{\log{\log{n}} + \log{4\pi}}{4 \log{n}} \Bigr) \\ 
    b_n &= (2 \log{n})^{-1/2}
  \end{align*}
  Then as $n \rightarrow \infty$ and $m = \Omega( \sqrt{n \log{n}})$,
  we have
  \begin{equation}
    \label{eq:14}
    \Delta_{\lambda}(t) \rightsquigarrow \begin{cases}
      \mathcal{G}(a_n \sigma_1 + \mu_1, b_n \sigma_1 ) & \text{for $t < t^{*}$.} \\
     \mathcal{G}(a_m \sigma_2 + \mu_2, b_m \sigma_2) &
     \text{for $t = t^{*}$.}
     \end{cases}
  \end{equation}
  where
  \begin{align*}
    \mu_1 &= (n-1)\langle \lambda, \pi_{00} \rangle \\
    \sigma_1 &= \sqrt{(n-1)\langle \lambda, \eta_{00} \lambda \rangle} \\
    \mu_2 &= \,\, (m - 1) \langle x, \pi_{11} \rangle + (n-
    m)\langle \lambda, \pi_{01} \rangle
    \\ \sigma_2 &= \sqrt{ (m - 1) \langle \lambda, \eta_{11} \lambda \rangle + (n -
      m) \langle \lambda, \eta_{01} \lambda \rangle}
    \end{align*}
\end{proposition}
\begin{lemma}
  \label{lem:2}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. For sufficiently large $n$ and sufficiently large
  $l$, the variable $T_{\lambda}^{l}(t)$ has approximately a
  $\mathcal{G}(\rho_{\lambda}, \varsigma_{\lambda})$ distribution with
\begin{align}
  \label{eq:13}
  \rho_{\lambda} &= \begin{cases}
    \frac{\sqrt{\pi}}{6} \frac{\mu_2 - \mu_1 + a_m\sigma_2 - (a_n + b_n
      \gamma)\sigma_1}{b_n \sigma_1} & \text{if $t =
        t^{*}$} \\
      - \frac{ \sqrt{\pi}}{6} \gamma & \text{if $t < t^{*}$}
  \end{cases}\\
  \varsigma_{\lambda} &= \begin{cases}
    \frac{\sqrt{\pi} b_m \sigma_2}{6 b_n \sigma_1} & \text{if $t =
      t^{*}$} \\
    \frac{\sqrt{\pi}}{6} & \text{if $t < t^{*}$}
    \end{cases}
\end{align}
$\gamma \approx 0.57721$ is the Euler-Mascheroni constant.
\end{lemma}
We are interested in finding the $\lambda$ that will maximize the
power of the test using $\Delta_{\lambda}$. The power approximation is
once again determined by various factors, but the dominating factor is
likely to be $\tfrac{\rho_\lambda}{\varsigma_\lambda}$. For sufficiently large $n$
and $m$, we also have
\begin{equation*}
  \begin{split}
  \frac{\rho_\lambda}{\varsigma_\lambda} &= (1 + o(1)) \frac{\mu_2 - \mu_1}{b_m
    \sigma_ 2} \\ &= \frac{\langle \lambda, (m-1)\pi_{11} + (n-m)\pi_{01} -
    (n-1) \pi_{00} \rangle}{\sqrt{\langle \lambda, ((m-1) \eta_{11} +
    (n-m)\eta_{01}) \lambda \rangle}}
  \end{split}
\end{equation*}
We can thus find the maximum and minimum of
$\tfrac{\rho_{\lambda}}{\varsigma_{\lambda}}$ by solving an 
eigenvalue problem as in Theorem~\ref{thm:1}. 
\subsection{Power estimates for $\Psi_{\lambda}(t)$}
The limiting distribution for the scan statistics for unattributed
random graphs was considered in
\cite{rukhin:_limit_distr_graph_scan_statis}. This subsection is
concerned with adpating the results in
\cite{rukhin:_limit_distr_graph_scan_statis} to attributed random
graphs. We state here only the relevant results. The proofs are
delegated to the appendices. \\ \\
\noindent
The limiting distribution for $\Psi_{\lambda}(t)$ is Gumbel for both
the case where $t < t^{*}$ and for $t = t^{*}$. To aid the exposition,
we first introduce some notations. Let $\lambda^{(2)}$ be the
element-wise square of $\lambda$. Define $C_{00}$, $C_{01}$, $C_{11}$
and $p_{00}$, $p_{01}$, $p_{11}$ to be
\begin{gather}
  \label{eq:19}
  C_{00} = \tfrac{\langle \lambda^{(2)}, \pi_{00} \rangle}{\langle \lambda,
    \pi_{00}\rangle}; \quad p_{00} = \tfrac{(\langle \lambda, \pi_{00}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{00} \rangle} \\
C_{11} = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{11}\rangle}; \quad p_{11} = \tfrac{(\langle \lambda, \pi_{11}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{11} \rangle} \\
C_{10} = \tfrac{\langle \lambda^{(2)}, \pi_{11} \rangle}{\langle \lambda,
    \pi_{10}\rangle}; \quad p_{10} = \tfrac{(\langle \lambda, \pi_{10}
    \rangle)^{2}}{\langle \lambda^{(2)},\, \pi_{10} \rangle} 
\end{gather}
Also let $z_n$ be defined, for any $n \in \mathbb{N}$, by
\begin{equation}
  \label{eq:20}
   z_n = \sqrt{2
      \log{n}}\Bigl(1 - \frac{\log{\log{n}} + \log{4 \pi}}{4
      \log{n}}\Bigr)
\end{equation}
\begin{theorem}
  \label{thm:2}
  Let $\Psi_{\lambda}(t)$ be the scan statistic for $t < t^{*}$. Let
  $N_{0}$, $a_n$, and $b_n$ be defined by
\begin{align*}
   \mu_0 &= (n-1) \langle 1, \pi_{00} \rangle \\
   \sigma_0 &= \sqrt{(n-1) (\langle 1, \pi_{00} \rangle - \langle
     1, \pi_{00} \rangle^2)} \\ 
    N_{0} &= \mu_0 + z_n \sigma_0 \\
    a_n &= N_{0} \frac{\langle \lambda, \pi_{00} \rangle}{\langle
      1, \pi_{00} \rangle} + C_{00}p_{00}\tbinom{N_{0}}{2} \\ 
   b_n &= (1 - \tfrac{C_{00}p_{00}}{2} + C_{00}p_{00} N_{0}) \frac{\sigma_0}{\sqrt{2 \log
     n}}
\end{align*}
Then we have
\begin{equation}
  \frac{\Psi_{\lambda}(t) - a_{n}}{b_n} \rightsquigarrow
  \mathcal{G}(0, 1)
\end{equation}
\end{theorem}%
We now move to the limiting distribution for $\Psi_{\lambda}(t^{*})$. 
Let us define $E$ and $F$ by
\begin{gather*}
 %  B \sim \mathrm{Bin}(m, \langle 1, \pi_{00} \rangle); \quad C \sim \mathrm{Bin}(n-m-1,
 % \langle 1, \pi_{01} \rangle) \\
  E \sim \mathrm{Bin}(m-1, \langle \pi_{11}, 1 \rangle) \\ F \sim \mathrm{Bin}(n-m,
  \langle \pi_{01}, 1 \rangle)
\end{gather*}%
For any random variable $X$, we let $\mu_X$ and $\sigma_X^{2}$ denote
the mean and variance of $X$. If $X$ and $Y$ are independent random
variables, we let $\mu_{X + Y}$ and $\sigma_{X + Y}^2$ denote the mean
and variance of the convolution $X + Y$. \\
\noindent Let $N_{\kappa}$ be the quantity defined by
\begin{equation}
  \label{eq:18}
  N_{\kappa} = \mu_{E + F} + z_m \sigma_{E + F}
\end{equation}
\begin{theorem}
  \label{thm:3}
  Let $a_{n,m}$ and $b_{n,m}$ be given by
  \begin{gather}
    \label{eq:28}
    \begin{split}
    a_{n,m} &=%  N_{\kappa} \frac{\langle x, \pi_{01} \rangle}{\langle
 %      1, \pi_{01} \rangle} \\
 % & +
    C_{00} p_{00} \Bigl(\tbinom{N_\kappa}{2} - \tbinom{\mu_E}{2} - \mu_E
    \mu_F\Bigr) \\ 
     & + C_{11} p_{11} \tbinom{\mu_E}{2} + C_{01} p_{01} \mu_E \mu_F 
    \end{split} \\
    b_{n,m} = (1 - \tfrac{C_{00} p_{00}}{2} + C_{00} p_{00}
    N_\kappa)\frac{\sigma_{E + F}}{\sqrt{2 \log{n_1}}}
  \end{gather}
  If $m = \Omega(\sqrt{n \log n})$, then
  \begin{equation}
    \label{eq:29}
    \frac{\Psi_{\lambda}(t^{*}) - a_{n,m}}{b_{n,m}}  \rightsquigarrow \mathcal{G}(0,1)  \end{equation}
\end{theorem}
\appendices
\section{Proofs of some stated results}
\label{sec:proofs-some-stated}
\begin{proof}[Theorem \ref{thm:1}]
  The maximizer of $\mu_\lambda$ also maximizes
  \begin{equation*}
    \mu_{\lambda}^{2} = \frac{\lambda^{T} \zeta \zeta^{T}
      \lambda}{\lambda^{T} \xi \lambda}
  \end{equation*}
  Because $\xi$ is positive definite, there exist a positive definite matrix
  $\xi^{1/2}$ such that $\xi^{1/2} \xi^{1/2} = \xi$. Letting $\nu = \xi^{1/2}
  \lambda$, the above expression can be rewritten as
  \begin{equation*}
    \mu_{\lambda}^{2} = \frac{\nu^{T} \xi^{-1/2} \zeta \zeta^{T}
      \xi^{-1/2} \nu}{ \nu^{T} \nu}
  \end{equation*}
  The claim then follows directly from the Rayleigh-Ritz theorem for
  Hermitean matrices.
\end{proof}
\begin{proof}[Proposition~\ref{prop:3}]
  Let $d_{\lambda}(v;t)$ be the (fused) degree of vertex $v$ at time
  $t$, i.e.,
  \begin{equation*}
    d_{\lambda}(v;t) = \sum_{k = 1}^{K} \lambda_k \sum_{w \in N(v)}
    \mathbb{I}\{ \phi(vw,t) = k \}
  \end{equation*}
  Let $\Gamma^{(k)}(v;t) = |\{w \colon \mathbb{I}\{\phi(wv,t) =
  k\}|$. The vector $\bm{\Gamma}(v;t) = (\Gamma^{(k)}(v;t))_{k=1}^{K}$
  for $t < t^{*}$ is distributed as a multinomial with $n-1$ trials
  and probability vector $\pi_{00}$. The following statements are
  made as $n \rightarrow \infty$ for fixed
  $K$. Now $d_{\lambda}(v;t) = \langle \lambda, \bm{\Gamma}(v;t)
  \rangle$ and
  thus, by the central limit theorem, we have
  \begin{equation}
    \label{eq:17}
    \frac{d_{\lambda}(v;t) - (n-1) \langle \lambda, \pi_{00}
      \rangle}{\sqrt{(n-1) \langle \lambda, \eta_{00} \lambda \rangle}}
      \rightsquigarrow \mathcal{N}(0, 1)
    \end{equation}
    We can thus consider the degree sequence of $G(t)$ for $t < t^{*}$
    as a sequence of {\em dependent} normally distributed random
    variables. By an argument analogous to the argument for
    Erd\"{o}s-Renyi random graphs in \cite[\S
    III.1]{bollobas85:_random_graph} we can show that the dependency
    among the $d_{\lambda}(v,t)$ can be ignored. $d_{\lambda}(v;t)$,
    when normalized, can thus be considered as a sequence of
    independent random variables from the standard normal in the
    limit. It is widely known that the sample maximum of standard
    normal random variables converges weakly to a Gumbel distribution
    \cite[\S 2.3]{galambos87:_asymp_theor_extrem_order_statis}. It is,
    however, not clear whether the convergence of
    $\Delta_{\lambda}(t)$ to a Gumbel distribution continues to hold
    under the composition of weak convergence as
    outlined above. We avoid this problem by showing
    directly that 
    \begin{equation}
      \label{eq:22}
  \mathbb{P}\Bigl(\tfrac{\Delta_{\lambda}(t) - (n-1)
        \langle \lambda, \pi_{00} \rangle}{\sqrt{(n-1) \langle
          \lambda, \eta_{00} \lambda \rangle}} \leq a_n + b_n x\Big)
      \rightarrow 
      e^{-e^{-x}} 
    \end{equation}
    Let $\zeta_v = \tfrac{d_{\lambda}(v;t) - (n-1) \langle \lambda,
      \pi_{00} \rangle}{\sqrt{(n-1) \langle \lambda, \eta_{00} \lambda
        \rangle}}$ and $F_n(u) = \mathbb{P}(\zeta_v \leq u)$. If $n
    \rightarrow \infty$ and $u = o(\sqrt{n})$, we have the
    following large deviations result \cite[Theorem~2, \S
    XVI.7]{feller71:_introd_probab_theor_its_applic}.
    \begin{equation}
      \label{eq:23}
      \frac{1 - F_{n}(u)}{1 - \Phi(u)} = \exp\Bigl(u^2
      h(\tfrac{u}{\sqrt{n}})\Bigr) \Bigl[1 + O(\tfrac{u}{\sqrt{n}}) \Bigr]
    \end{equation}
    where $h(z)$ is a power series defined by
    \begin{equation*}
      z^{2} h(z) = h_0 z^3 + h_1 z^4 + \dots
    \end{equation*}
    and the $h_i$ are constants. Letting $u_n = a_n + b_n x$ in
    Eq.~\eqref{eq:23}, we have
    \begin{equation}
      \label{eq:24}
     1 - F_n(u_n) = (1 - \Phi(u_n))(1 + C \tfrac{u_n^{3}}{\sqrt{n}} +
     O(\tfrac{u_n^{4}}{n}))
    \end{equation}
    for some constant $C$. We therefore have
    \begin{equation}
      \label{eq:25}
      \begin{split}
      \mathbb{P}(\max_{v \in [n]} \zeta(v) \leq u_n) &= (F_n(u_n))^{n}
      \\
      &= \Bigl[\Phi(u_n) + \Phi(u_n) 
      \tilde{O}(\tfrac{1}{\sqrt{n}})\Bigr]^{n} \\
      &= \Bigl[\Phi(u_n) + \tilde{O}(\tfrac{1}{n^{3/2}})\Bigr]^{n} \\
      &= (\Phi(u_n))^{n} + \tilde{O}(\tfrac{1}{n^{1/2}}) \\
      & \rightarrow e^{-e^{-x}} 
      \end{split}
    \end{equation}
    where $\tilde{O}$ denote that the powers of $u_n =
    O(\log{n})$ are ignored. Eq.~\eqref{eq:22} is established and
    we obtain the limiting Gumbel distribution for
    $\Delta_{\lambda}(t)$ for $t < t^{*}$ in Eq.~\eqref{eq:14}.
    % Now $n(1 - \Phi(u_n)) = \exp(-x)(1 + o(u_n^{-1 + \theta}))$ for
    % some small $\theta > 0$. We thus have
    % \begin{equation}
    %   \label{eq:26}
    %   \begin{split}
    %   \mathbb{P}(\max_{v \in [n]} \zeta(v) \leq u_n) &= \Bigl[ \Phi(u_n) - C
    %   u_n^{3}n^{-3/2} e^{-x}(1 + o(u_n^{-1 + \theta})) + O(u_n^{4}/n)
    %   n^{-1} e^{-x} \Bigr]^{n} \\
    %   &= (\Phi(u_n))^{n}\Bigl[1 - C u_n^{3}n^{-1/2} e^{-x}(n \Phi(u_n))^{-1} +
    %   o(u_n^{-4 + \theta})n^{-2} e^{-x}\Bigr]^{n} \\
    %   &= (\Phi(u_n))^{n} - C u_n^{3}n^{-1/2} e^{-x} (\Phi(u_n))^{n-1} +
    %   o(u_n^{-4 + \theta}n^{-1/2}) e^{-x} \\
    %   &= (1 + o(1)) \Phi(u_n)^{n}
    %   \end{split}
    % \end{equation}
    % where the term $o(1)$ can be made to be independent of
    % $x$. Furthermore, we can verify that $\Phi(u_n)^{n} \rightarrow
    % e^{e^{-x}}$ and thus Eq.~\eqref{eq:22} is established. 
 % Let us define
 %  $\xi_s$ for $s \in \mathbb{R}$ as the number of vertices $v$ such
 %  that $\tfrac{d_\lambda(v;t) - (n-1) \langle \lambda, \pi_{00}
 %    \rangle}{\sqrt{(n-1) \langle \lambda, \eta_{00} \lambda \rangle}}
 %  \geq s$. Note that $\xi_s \leq k$ only if there are at most $k$
 %  vertices with $\tfrac{d_{\lambda}(v;t) - (n-1) \langle \lambda,
 %    \pi_{00} \rangle}{\sqrt{(n-1) \langle \lambda, \eta_{00} \lambda
 %      \rangle}} \geq s$. Thus, $\tfrac{\Delta_{\lambda}(v;t) - (n-1)
 %    \langle \lambda, \pi_{00} \rangle}{\sqrt{(n-1) \langle \lambda,
 %      \eta_{00} \lambda \rangle}} < s$ only if $\xi_s = 0$. Because
 %  the sample maximum of Gaussian random variables converges weakly to
 %  a Gumbel \cite[\S 2.3]{galambos87:_asymp_theor_extrem_order_statis},
 %  by taking $n \rightarrow \infty$ followed by $s \rightarrow \infty$,
 %  we can show that $\tfrac{\Delta_{\lambda}(v;t) - (n-1) \langle \lambda,
 %  \pi_{00} \rangle}{\sqrt{(n-1) \langle \lambda, \eta_{00} \lambda
 %    \rangle}} \rightsquigarrow \mathcal{G}(a_n, b_n)$. 
The case when $t = t^{*}$ can be
derive in a similar manner. We first show that if $m =
\Omega(\sqrt{n \log{n}})$, then $\Delta_{\lambda}(v;t^{*}) \rightsquigarrow
\max_{v \in [m]}{d_{\lambda}(v;t^{*})}$ \cite[Lemma
3.1]{rukhin:_limit_distr_graph_scan_statis}.  
We then show, again by the 
central limit theorem that for $v \in [m]$, $\tfrac{d_{\lambda}(v;t^{*}) -
  \mu_2}{\sigma_2} \rightsquigarrow N(0,1)$. It then follows, similar
to our previous reasoning for the case where $t < t^{*}$, that
$\max_{v \in [m]} \tfrac{d_{\lambda}(v;t^{*}) -
  \mu_2}{\sigma_2} \rightsquigarrow \mathcal{G}(a_m,b_m)$ and we obtain
Eq.~\eqref{eq:14} for $t = t^{*}$.  
\end{proof}
\begin{proof}[Lemma~\ref{lem:2}]
  Let $ X \sim \mathcal{G}(\alpha, \beta)$. We consider the
  normalization $\tfrac{X - \mu}{\sigma}$. We have
  \begin{equation*}
    \begin{split}
    \mathbb{P}[ \frac{X - \mu}{\sigma} \leq z]  &= \mathbb{P}[X \leq z
    \sigma + \mu] 
    = e^{-e^{-(z \sigma + \mu - \alpha)/\beta}} \\
      &= e^{- e^{-(z - (\alpha - \mu)/\sigma)/(\beta/\sigma)}}
    \end{split}
  \end{equation*}
  Thus, $\tfrac{X - \mu}{\sigma} \sim \mathcal{G}(\tfrac{\alpha -
    \mu}{\sigma}, \tfrac{\beta}{\sigma})$. Because the sample
  mean and the sample variance are consistent estimators, the claims
  follow after an application of Slutsky's theorem.
\end{proof}
\begin{proof}[Theorem~\ref{thm:2}]
Let $\phi_{\lambda}(v;t) = \psi_{\lambda}(v;t) - d_{\lambda}(v;t)$ be
the (fused) locality statistics for
vertex $v$ at time $t$ not including the (fused) degree of $v$, i.e.,
\begin{equation}
  \label{eq:11}
  \phi_{\lambda}(v;t) = \sum_{k=1}^{K} \lambda_k \sum_{\substack{uw
      \in N(v) \\ u,w \not = v}}
  \mathbb{I}\{\phi(uw,t) = k\}.
\end{equation}
The following statements are made conditional on $|N(v)| = l$. First
of all, we have
\begin{equation*}
  \phi_{\lambda}(v;t) = \sum_{k=1}^{K}{\lambda_k z_k}
\end{equation*}
where the $(z_1, \dots, z_K)$ are distributed as
\begin{gather*}
  (z_1,z_2,\dots,z_K) \sim \textrm{multinomial}\Bigl(
  \tbinom{l}{2}, \pi_{00}\Bigr) 
\end{gather*}
By the central limit theorem, we have
\begin{equation*}
  \frac{\phi_\lambda(v;t) - \tbinom{l}{2} \langle \lambda, \pi_{00}
    \rangle}{\sqrt{\tbinom{l}{2} \langle \lambda, \eta_{00} \lambda \rangle}}
 \rightsquigarrow \mathcal{N}(0,1) 
\end{equation*}
Let $Y_{l} = C_{00} \mathrm{Bin}(\tbinom{l}{2}, p_{00})$. Then $\mathbb{E}[Y_l] = \tbinom{l}{2} \langle \lambda, \pi_{00} \rangle$
and $\mathrm{Var}[Y_l] = \tbinom{l}{2} \langle \lambda,
\eta_{00} \lambda \rangle$ and again by the central limit theorem, we have
\begin{equation}
  \label{eq:12}
  \frac{\psi_{\lambda}(v;t) - \tbinom{l}{2} \langle \lambda, \pi_{00}
    \rangle}{\sqrt{\tbinom{l}{2} \langle \lambda, \eta_{00} \lambda
      \rangle}} \rightsquigarrow \frac{Y_l - \tbinom{l}{2} \langle \lambda, \pi_{00} \rangle}{\sqrt{\tbinom{l}{2}
    \langle \lambda, \eta_{00} \lambda \rangle}}
\end{equation}
Eq.~\eqref{eq:12} states that the locality statistics for our
attributed random graphs model with $t < t^{*}$
can be approximated by the locality statistics for an Erd\"{o}s-Renyi
graph with edge probability $p_{00}$. Theorem~\ref{thm:2} then follows
from Theorem~1.1 in \cite{rukhin:_limit_distr_graph_scan_statis}.
\end{proof}
\begin{proof}[Theorem~\ref{thm:3}]
  Let $\phi_{\lambda}(v;t^{*}) = \psi_{\lambda}(v;t^{*}) -
  d_{\lambda}(v;t^{*})$. Let $d_{\zeta}(v;t)$ be
the number of neighbors of $v$ that lies in $[m]$ and
$d_{\xi}(v;t)$ be the number of neighbors of $v$ that lies in $[n]
\setminus [m]$. The following statements are made, conditional on $d_{\zeta}(v;t^{*}) =
l_{\zeta}$ and $d_{\xi}(v;t^{*}) = l_{\xi}$. First of all, we have
\begin{equation}
  \phi_{\lambda}(v;t^{*}) = \sum_{k=1}^{K} \lambda_k ( y^{(\zeta)}_k +
  y^{(\xi)}_k + y^{(\omega)}_k)
\end{equation}
where $(y^{(\zeta)}_1, \dots, y^{(\zeta)}_K)$, $(y^{(\xi)}_1,\dots,
 y^{(\xi)}_K)$,  $(y^{(\omega)}_1, \dots, y^{(\omega)}_K)$ are
 distributed as
\begin{gather*}
(y^{(\zeta)}_1,\dots,y^{(\zeta)}_K) \sim \textrm{multinomial}\Bigl(
\tbinom{l_\zeta}{2}, \pi_{11}\Bigr) \\ 
(y^{(\xi)}_1,\dots,y^{(\xi)}_K) \sim \textrm{multinomial}\Bigl(
\tbinom{l_\xi}{2}, \pi_{00}\Bigr) \\
(y^{(\omega)}_1,\dots,y^{(\omega)}_m) \sim \textrm{multinomial}\Bigl(
l_\zeta l_\xi, \pi_{10}\Bigr)
\end{gather*}
Let $\rho$ and $\varsigma$ be defined as
\begin{gather*}
  \rho = \langle \lambda, \tbinom{l_{\zeta}}{2} \pi_{11} +
  \tbinom{l_{\xi}}{2} \pi_{00} + l_{\zeta} l_{\xi} \pi_{10} \rangle \\
  \varsigma = \langle \lambda, \Bigl(\tbinom{l_{\zeta}}{2} \eta_{11} +
  \tbinom{l_{\xi}}{2} \eta_{00} + l_{\zeta} l_{\xi} \eta_{10}\Bigr) \lambda
  \rangle
\end{gather*}
By the central limit theorem, we have
\begin{equation}
  \label{eq:16}
  \frac{\phi_{\lambda}(v;t^{*}) - \rho}{\varsigma} \rightsquigarrow
  \mathcal{N}(0,1)
\end{equation}
Let $Y_{\zeta} \sim C_{11} \mathrm{Bin}\Bigl(\tbinom{l_{\zeta}}{2},
p_{11}\Bigr)$, $Y_{\xi} \sim C_{00} \mathrm{Bin}\Bigl(\tbinom{l_\xi}{2},
p_{00}\Bigr)$ and $Y_{\omega} \sim C_{10} \mathrm{Bin}\Bigl( l_{\zeta}
l_{\xi}, p_{10} \Bigr)$. By the central limit theorem, we have
\begin{equation}
  \label{eq:21}
 \frac{\phi_{\lambda}(v;t^{*}) - \rho}{\varsigma} \rightsquigarrow
 \frac{Y_{\zeta} + Y_{\xi} + Y_{\omega} - \rho}{\varsigma}
\end{equation}
Eq.~\eqref{eq:21} states that the locality statistics for our
attributed random graphs model time $t = t^{*}$ can be approximated by
the locality statistics for an unattributed kidney and egg model. We
thus define $Y(v) = Y_{\zeta}(v) + Y_{\xi}(v) + Y_{\omega}(v)$
where $Y_{\zeta}(v) \sim C_{11}
\mathrm{Bin}(\tbinom{d_{\zeta}(v;t)}{2}, p_{11})$ and $Y_{\xi},
Y_{\omega}$ are defined accordingly. Let $G \sim \kappa(n,m,p_{11},
p_{10}, p_{00})$ be an unattributed kidney-egg graph with the
probability of egg-egg, egg-kidney, and kidney-kidney connections
being $p_{11}$, $p_{10}$, and $p_{00}$, respectively. Let $M(v)$ and
$W(v)$ denote the number of vertices adjacent to $v$ in $[m]$ and $[n]
\setminus [m]$, respectively. $D(v) = M(v) + W(v)$ is then the degree
of $v$ in $G$. We now show two
inequalities relating the tail distribution of 
$\Delta(G)$ and $\Upsilon(G) = \max_{v \in V(G)} Y(v)$.
\begin{gather}
  \label{eq:27}
    \limsup \mathbb{P}( \Upsilon(G) \geq a_{n,m} ) \leq \lim
   \mathbb{P}( \Delta(G) \geq N_\kappa) \\
   \label{eq:30}
  \liminf \mathbb{P}( \Upsilon(G) \geq a_{n,m} ) \geq \lim \mathbb{P}(
  \Delta(G) \geq N_{\kappa})
\end{gather}
\begin{proof}[Eq.~\eqref{eq:27}]
 Let $C^{*} = \max\{C_{11},
C_{10}, C_{00}\}$ and $d_{n,m} = \sqrt{2 a_{n,m}/C^{*}}$. We first
note that
\begin{equation*}
\Upsilon(G) \geq a_{n,m} \Rightarrow C^{*} \tbinom{D(v)}{2} \geq
a_{n,m} \Rightarrow D(v) \geq d_{n,m} 
\end{equation*}
Let us define $h(v) = \mathbb{E}[Y(v)]$, i.e., 
\begin{equation*}
  \begin{split}
  h(v) = C_{00}p_{00} \tbinom{D(v)}{2} &+ (C_{11} p_{11} - C_{00}
  p_{00}) \tbinom{M(v)}{2} \\ &+ (C_{10} p_{10} - C_{00} p_{00}) M(v) W(v)
  \end{split}
\end{equation*}
We then have
  \begin{equation*}
    \label{eq:31}
    \begin{split}
    \mathbb{P}(\Upsilon(G) \geq a_{n,m}) &= \mathbb{P}\Bigl( \bigcup_{v
      \in V(G)} Y(v) \geq a_{n,m}\Bigr) \\
    &= \mathbb{P}\Bigl( \bigcup_{v
      \in V(G)} Y(v) \geq a_{n,m}, \, D(v) \geq d_{n,m} \Bigr) \\
    &= P_1 + P_2
    \end{split}
  \end{equation*}
  where 
  \begin{align*}
    \vartheta_n &= C_{00} \Bigl[ \tbinom{n}{2} p_{00} (1 -
    p_{00})\Bigr]^{1/2} \log{n} \\
    P_1 &= \mathbb{P}(\bigcup_{v \in V(G)} D(v) \geq d_{n,m}, h(v) \geq
    a_{n,m} - \vartheta_n) \\
    P_2 &= \mathbb{P}(\bigcup_{v \in V(G)} D(v) \geq d_{n,m},
    Y(v) - h(v) \geq \vartheta_n)
  \end{align*}
  We now show that $P_2$ is negligible as $n \rightarrow \infty$. To
  proceed, let $p_{e,f} = \mathbb{P}(M(v) = e, W(v) =
f)$. $P_2$ can then be bounded as follows
\begin{equation*}
  \begin{split}
    P_2 & \leq n \sum_{D(v) \geq
        d_{n,m}}{\mathbb{P}\biggl( Y(v) - \mathbb{E}[Y(v)] \geq
        \vartheta_n \biggr) p_{e,f}}
  \\
  &= n \sum_{D(v) \geq d_{n,m}}{\mathbb{P}\biggl(
    \tfrac{Y(v) - \mathbb{E}[Y(v)]}{\sqrt{\mathrm{Var}[Y(v)]}} \geq
    \tfrac{\vartheta_n}{\sqrt{\mathrm{Var}[Y(v)]}} \biggr) p_{e,f}} \\
    &\leq n \sum_{D(v) \geq d_{n,m}} (1 + o(1)) \mathbb{P}(Z \geq \Theta(\log{n})) p_{e,f} \\
    &= o(1)
  \end{split}
\end{equation*}

\end{proof}
\end{proof}
\bibliography{ssp2011}
\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
