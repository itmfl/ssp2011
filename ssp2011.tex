\documentclass[draftcls]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mathrsfs}
\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\usepackage[colorlinks=true,pagebackref,linkcolor=magenta]{hyperref}
\usepackage[colon,sort&compress]{natbib}
%\numberwithin{equation}{section}
\renewcommand\arraystretch{1.2}
\let\underbrace\LaTeXunderbrace
\let\overbrace\LaTeXoverbrace
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\bibliographystyle{IEEEtran}
\begin{document}
\title{Attribute fusion in a latent process model for time series of
  graphs}
\author{Carey~E.~Priebe, Nam~H.~Lee, Youngser~Park, and Minh~Tang}%
%\thanks{Johns Hopkins University \\ Department of Applied Mathematics
%  and Statistics \\ Baltimore, Maryland 21218-2682 USA}%
\maketitle
\begin{abstract}
 We consider the problem of anomaly/change point detection given a
 time series of graphs with categorical attributes on the
 edges. Various attributed graph invariants are considered, and their
 power for detection as a function of a linear fusion parameter is
 presented.  
\end{abstract}
\begin{IEEEkeywords}
  Anomaly detection, Attributed Random Graphs, Fusion, Random Dot
  Product Graphs
\end{IEEEkeywords}
\section{Introduction}
\subsection{Latent Process Model}
The latent process model for time series of attributed graphs was
originally presented in
\cite{lee:_laten_proces_model_time_attrib_random_graph}. We summarized
here the ideas that are relevant to our discussion. We begin by
introducing some terminology. Let $\mathscr{S}$ be the unit simplex in
$\mathbb{R}^{K}$, i.e.,
\begin{equation}
  \mathscr{S} = \{ \xi \in [0,1]^{K}
  \colon \sum_{k = 1}^{K} \xi_k \leq 1 \}.
\end{equation}
A {\em random dot product space} for attributed graphs with vertices
in $[n]$ and edge attributes $\mathscr{K} = [K]
$ is then a pair $(\mathbf{X},G)$ of random elements such that
\begin{enumerate}
\item $\mathbf{X} = \{X_i\}_{i = 1}^{n}$ is a collection of
  $\mathscr{S}$-valued random vectors.
\item $G$ is a random graph with vertices set $[n]$ such that
  \begin{equation}
    \label{eq:1}
    \mathbb{P}(i \sim j \,|\, \mathbf{X} = (x_1, x_2, \dots,
    x_n)) = \langle x_i, x_j \rangle.
  \end{equation}
  and that $\mathbf{P}(i \sim j \,|\, \mathbf{X})$ and $\mathbf{P}(i' \sim
  j' \,|\, \mathbf{X})$ are independent whenever $(i,j) \not = (i',j')$. If
  $i \sim j$ in $G$, then the attribute of the edge $\{i,j\}$ is an
  element of $\mathscr{K}$. In particular, $\{i,j\}$ has attribute $k$
  with probability $x_{i,k} x_{j,k}$. 
\end{enumerate}
Let $\mathscr{K}_{+} = \{1,\dots,K+1\}$. We say that a c\'{a}dl\'{a}g
process $W \colon [0,\infty) \mapsto \mathscr{K}_{+}^{n}$ induces the
sequence of random dot product spaces $\mathscr{V} = \{X(t), G(t)\}_{t
  = 1}^{\infty}$ if
\begin{enumerate}
\item Each $(X(t), G(t))$ is a random dot product space with vertices
  $[n]$ and attributes $\mathscr{K}$. Furthermore, for each
  $i \in [n]$, $k \in \mathscr{K}$ and $t \in \mathbb{N}$, we have
  $X_{i,k}(t)  = \int_{t - 1}^{t}{ \mathbf{1}\{W_i(u) = k\}\, du}$.
\item  For each $t \in \mathbb{N}$, we have
  \begin{equation}
    \label{eq:2}
    \mathbb{P}(G(t) = g \,|\, \mathscr{F}_{\leq t}) = \mathbb{P}(G(t) = g \,|\, X(t))
  \end{equation}
where $\mathscr{F}_{\leq t}$ are the sigma fields generated by $\{W(s)
  \colon s \leq t\}$.
\end{enumerate}
We will call any pair $(\mathscr{V}, W)$ that satisfies the above
properties a random dot process model.  In particular, we are
interested in the pairs $(\mathscr{V}, W)$ possessing the following
properties:
\begin{enumerate}
\item For each $t \in \mathbb{N}$ and vertex $i \in [n]$, there exists
  a matrix $\mathbf{Q}^{(i)}(t)$ such that $W_i$, when restricted to
  the interval $[t, t+1)$, is a stationary, continuous-time Markov
  chain with state space $\mathscr{K}_+$, intensity matrix
  ${\mathbf{Q}^{(i)}(t)}$, and stationary distribution
  $\pi^{(i)}(t)$. $W$ is thus a stationary, continuous-time Markov chain with state
  space $\mathscr{K}_{+}^{n}$ and intentisy matrix
  $\otimes_{i=1}^{n}\mathbf{Q}^{(i)}(t)$.
\item There exists a $t^{*} \in \mathbb{N}$ and a $m < n$ such that 
  \begin{enumerate}
  \item for  $t < t^{*}$
    \begin{gather*}
      \pi^{(i)}(t) \equiv \pi_0 \\
      Q^{(i)}(t) \equiv \mathbf{Q}_0
    \end{gather*}
  \item  for $t \geq t^{*}$
    \begin{gather*}
      \pi^{(1)}(t) = \dots = \pi^{(m)}(t) = \pi_1 \\
      \pi^{(m+1)}(t)  = \dots = \pi^{(n)}(t) = \pi_0 \\
      \mathbf{Q}^{(1)}(t)  = \dots = \mathbf{Q}^{(m)}(t) = \mathbf{Q}_1 \\
      \mathbf{Q}^{(m+1)}(t) = \dots = \mathbf{Q}^{(n)}(t) = \mathbf{Q}_0 
    \end{gather*}
  \end{enumerate}
\end{enumerate}
The above properties characterize a random dot process model with a
change-point phenomena. We will refer to $(t^{*}, m, \pi_0, \pi_1,
\mathbf{Q}_0, \mathbf{Q}_1)$ as the change parameters. Because $W$ is completely
determined by the change parameters, we often choose to omit $W$ and
only mention $\mathscr{V}$ when referring to random dot process models
with change-point phenomena. For a rdpm $\mathscr{V}$ with change
parameters $(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$, we can construct
several approximations to $\mathscr{V}$. Of particular interests are the
following two approximations.
\begin{definition}
  \label{def:1}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The first order approximation
  $\bar{\mathscr{V}}$ of $\mathscr{V}$ is the sequence $\{(\bar{X}(t),
  \bar{G}(t)\}_{t = 1}^{\infty}$ of independent random dot product
  spaces such that
 \begin{enumerate}
 \item For $t < t^{*}$,
   \begin{equation}
     \label{eq:5}
     \bar{X}_{i}(t) \equiv \bar{\pi}_0.
   \end{equation}
 \item For $t \geq t^{*}$
   \begin{gather*}
     \bar{X}_{i}(t) \equiv \bar{\pi}_1 \quad \text{for $i \leq m$} \\
     \bar{X}_{i}(t) \equiv \bar{\pi}_0 \quad \text{for $i > m$} 
   \end{gather*}
 \end{enumerate}
 where $\bar{\pi}_0$ and $\bar{\pi}_1$ are sub-probability vectors
 obtained by removing the last coordinate of $\pi_0$ and $\pi_1$. 
\end{definition}
\begin{definition}
  \label{def:2}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, Q_0, Q_1)$. Define $\mathbf{Z}_0$ and $\mathbf{Z}_1$
  by
  \begin{gather}
    \mathbf{Z}_0 = (\mathbf{1}\mathbf{\pi}_0^{T} -
    \mathbf{Q}_0)^{-1}(\mathbf{I} - \mathbf{1}\pi_0^{T}) \\
    \mathbf{Z}_1 = (\mathbf{1}\mathbf{\pi}_1^{T} -
    \mathbf{Q}_1)^{-1}(\mathbf{I} - \mathbf{1}\pi_1^{T})
  \end{gather}
  $\mathbf{Z}_0$ and $\mathbf{Z}_1$ are the fundamental matrices
  for the continuous-time Markov chain on $\mathscr{K}$ with intensity
  matrix $\mathbf{Q}_0$ and $\mathbf{Q}_1$ (see
  e.g., \cite[p. 55]{asmussen03:_applied_probab_queues}). Let
  $\Sigma_0$ and $\Sigma_1$ be given by
  \begin{gather*}
    \Sigma_0 = \mathrm{diag}(\pi_0) \mathbf{Z}_0 + \mathbf{Z}_0^{T}
    \mathrm{diag}(\pi_0) \\
    \Sigma_1 = \mathrm{diag}(\pi_1) \mathbf{Z}_1 + \mathbf{Z}_1^{T}
    \mathrm{diag}(\pi_1)
  \end{gather*}
  A second order approximation $\hat{\mathscr{V}}$ of $\mathscr{V}$
  is the sequence $\{\hat{X}(t), \hat{G}(t)\}_{t=1}^{\infty}$ where
  \begin{enumerate}
  \item For each $t$ and each $i \in [n]$, $\hat{X}^{i}(t)$ is a
    random vector obtained by truncating a multivariate normal random
    vector $Z_{i}(t)$ to $\mathscr{S}$.
  \item For $t < t^{*}$,
    \begin{gather}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0 \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma^{0}}
    \end{gather}
  \item For $t \geq t^{*}$,  
    \begin{gather*}
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_0, \quad \text{for $i \leq
        m$}  \\
      \mathbb{E}[Z_i(t)] \equiv \bar{\pi}_1, \quad \text{for $i > 
        m$}  \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{0} \quad \text{for $i
        \leq m$} \\
      \mathrm{Var}[Z_i(t)] \equiv \hat{\Sigma}_{1} \quad \text{for $i
        > m$}
    \end{gather*}
    where $\hat{\Sigma}_0$ and $\hat{\Sigma}_1$ are the matrices
    obtained by removing last row and column of $\Sigma_0$ and
    $\Sigma_1$. 
  \end{enumerate}
\end{definition}
\section{Change-point detection}
\subsection{Graphs Invariants}
In this paper we consider the problem of detecting chatter anomalies
in time series of graphs using attributed invariants. The particular
invariants of interests are the size, max degree, scan, and number of
triangles. Specifically, we consider linear attribute fusion with
parameter $\lambda \in \mathbb{R}^{K}_{\geq 0}$ via
\begin{gather}
  \label{eq:6}
  \mathrm{size}_{\lambda}(G(t)) = \sum_{k=1}^{K} \lambda_k \sum_{u,v}
  \mathbf{1}\{ \phi(uv,t) = k \} \\
  \Delta_{\lambda}(G(t)) = \max_{v \in V} \, \sum_{k = 1}^{K} \lambda_k
  \sum_{u \in N(v)}{\mathbf{1}\{\phi(uv,t) = k\}} \\
  \mathrm{scan}_{\lambda}(G(t)) = \max_{v \in V} \, \sum_{k = 1}^{K}
  \lambda_k \sum_{u,w \in N(v)} \mathbf{1}\{\phi(uw,t) = k\} \\
  \label{eq:3}
  \tau_\lambda(G(t)) = \sum_{k=1}^{K} \lambda_k \sum_{(u,v,w) \in
    \binom{V}{3}} h_k(u,v,w)
\end{gather}
where $h_k(u,v,w)$ is the indicator function that the argument, a set
of three vertices, forms a triangle (3-clique) of type $k$. A triangle
is of type $k$ provided that all edges in the triangle have attribute
$k$. \\ \\
\noindent
Let $J_{\lambda}(t)$ be a statistic of the form as in Eq.~\eqref{eq:6}) through
Eq.~\eqref{eq:3}. We define the normalization $\bar{J}^{m}_\lambda(t)$
of $J_{\lambda}$ based on recent past as
\begin{equation}
  \label{eq:4}
 \bar{J}^{m}_{\lambda}(t) = \frac{1}{m}\sum_{s = 1}^{m} J_{\lambda}(t - s) 
\end{equation}
where $m \in \mathbb{N}$ specified the width of the running-average
window. The main object of our interests are then the normalized fusion
statistic $T_{\lambda}^{m}(t)$ as defined by
\begin{equation}
  \label{eq:7}
 T_{\lambda}^{m}(t) = % \begin{cases}
   % J_{\lambda}(t) - J_{\lambda}(t - 1) & \text{if $m = 1$} \\
   \frac{J_{\lambda}(t) -
     \bar{J}_{\lambda}^{m}(t)}{\sqrt{\tfrac{1}{m-1}
       \sum_{s=1}^{m}(J_{\lambda}(t - s) - \bar{J}_{\lambda}^{m}(t))^2}}
%   & \text{if $m \geq 2$}
 %  \end{cases}
\end{equation}
for $m \geq 2$. 
\begin{theorem}
  \label{thm:1}
  Let $\lambda^{*}$ be the eigenvector corresponding to the largest
  eigenvalue of $ \mathbf{A}^{-1/2} \mathbf{B} \mathbf{A}^{-1/2}$,
  i.e., 
 \begin{equation}
   \label{eq:9}
  \lambda^{*} = \argmax_{\lambda \colon \| \lambda \| = 1}
  \lambda^{T} \mathbf{A}^{-1/2} \mathbf{B} \mathbf{A}^{-1/2}
  \lambda.
 \end{equation}
 We then have
 \begin{gather}
   \label{eq:8}
 \argmax_{\lambda \in \mathcal{B}}
 \mu_{\lambda} \, \cap \, \{\lambda^{*}, - \lambda^{*}\} \not = \emptyset \\
 \argmin_{\lambda \in \mathcal{B}} \mu_{\lambda} \, \cap \, \{\lambda^{*}, -
 \lambda^{*}\} \not = \emptyset.
 \end{gather}
\end{theorem}
\bibliography{ssp2011}
\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
