\documentclass[10pt,twocolumn]{IEEEtran}
\usepackage{fontenc}
\pdfminorversion=4
%\usepackage{lmodern}
%\usepackage[utopia]{mathdesign}
\usepackage{graphicx}
%\usepackage{xltxtra}
%\setmainfont[Mapping=tex-text]{Linux Libertine}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500
\usepackage{array}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage[tight,normalsize,sf,SF]{subfigure}
\ifCLASSOPTIONcompsoc
\usepackage[font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[font=footnotesize]{subfig}
\fi
\usepackage{subfig}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage{bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\usepackage[colorlinks=true,pagebackref,linkcolor=magenta]{hyperref}
\usepackage[noadjust]{cite}
\renewcommand\arraystretch{1.2}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\def\mathllap{\mathpalette\mathllapinternal}
\def\mathllapinternal#1#2{%
\llap{$\mathsurround=0pt#1{#2}$}% $
}
\def\clap#1{\hbox to 0pt{\hss#1\hss}}
\def\mathclap{\mathpalette\mathclapinternal}
\def\mathclapinternal#1#2{%
\clap{$\mathsurround=0pt#1{#2}$}%
}
\def\mathrlap{\mathpalette\mathrlapinternal}
\def\mathrlapinternal#1#2{%
\rlap{$\mathsurround=0pt#1{#2}$}% $
}
\bibliographystyle{IEEEtran}
\begin{document}
\title{Attribute fusion in a latent process model for time series of
  graphs}
\author{Minh~Tang, Youngser~Park, Nam~H.~Lee, and Carey~E.~Priebe*%
\thanks{Minh~Tang is with the Department of Applied Mathematics and
  Statistics, Johns Hopkins University, Baltimore, MD 21218
  USA. (email: mtang10@jhu.edu)}%
\thanks{Youngser~Park is with the Center of Imaging Science, Johns
  Hopkins University, Baltimore, MD 21218
  USA. (email:youngser@jhu.edu)}%
\thanks{Nam~H.~Lee is with the Department of Applied Mathematics and
  Statistics, Johns Hopkins University, Baltimore, MD 21218
  USA. (email: nhlee@jhu.edu)}%
\thanks{Carey~E.~Priebe is with the Department of Applied Mathematics and
  Statistics, Johns Hopkins University, Baltimore, MD 21218
  USA. (phone: 410-516-7200; fax:410-516-7459; email: cep@jhu.edu)}}%
%\markboth{IEEE Transactions on Signal Processing}{}
% \begin{IEEEkeywords}
%   Anomaly detection, Attributed Random Graphs, Random Dot
%   Product Graphs.
% \end{IEEEkeywords}}

\maketitle
\begin{abstract}
  Hypothesis testing on time series of attributed graphs has
  applications in diverse areas, e.g., social network analysis
  (wherein vertices represent individual actors or organizations),
  connectome inference (wherein vertices are neurons or brain regions)
  and text processing (wherein vertices represent authors or
  documents).  We consider the problem of anomaly/change point
  detection given the latent process model for time series of graphs
  with categorical attributes on the edges presented in \cite{lee11}.
  Various attributed graph invariants are considered, and their power
  for detection as a function of a linear fusion parameter is
  presented.  Our main result is that inferential performance in
  mathematically tractable first-order and second-order approximation
  models does provide guidance for methodological choices applicable
  to the exact (realistic but intractable) model. Furthermore, to the
  extent that the exact model is realistic, we may tentatively
  conclude that approximation model investigations have some bearing
  on real data applications.
\end{abstract}
\ifCLASSOPTIONpeerreview
\begin{center} \bfseries EDICS Category: SSP-SSAN, SSP-DETC \end{center}
\fi
%\pagebreak
\section{Introduction}
\label{sec:introduction}
The notion of data as a time series is fundamental to signal
processing. However, time-series analysis had been mostly confined to
data in Euclidean spaces, while the representation of data as graphs
with the vertices as the entities and the edges as the relationships
between the entities is now ubiquitous in many application domains,
e.g., social networks (wherein vertices represent individual actors or
organizations), neuroscience (wherein vertices are neurons or brain
regions), and text processing (wherein vertices represent authors or
documents). Furthermore, in many of these application domains, the
edges between the vertices, in addition to representing the presence
of a relationship, can also be use to identify the attributes
associated with that relationship. For example, in social network
graphs where edges represent email communications, the attributes
signify the context of the email messages. Finally, in many of these
application domains, the underlying data changes over time, and thus a
representation of data as a time-series of attributed graphs is
natural. The analysis of time-series of (attributed) graphs thus
presents a natural and challenging extension to the signal processing
community. 

We mention here some related work. The notion of signal
processing for graphs appears to be in an emerging
state, with significant research activity in constructing wavelet
representations for graphs, e.g., 
\cite{hammond10:_wavel,r.coifman06:_diffus,crovella03:_graph,narangss:_perfec}. 
The formulation of graph analysis problems in a signal processing framework
is also of potential interest, e.g, \cite{miller10:_towar_euclid}. 

One of the main exploitation tasks in time-series analysis is the
problem of anomaly/change-point detection, where anomaly is broadly
interpreted to mean deviation from some ``normal'' pattern and
change-point is the time window at which the anomalous deviation
occurs. In the context of a time-series of graphs, an anomaly can be
for example the presence of a specific subgraph, or frequent occurence
of some subgraphs, or other more vague notions such as existence of a
community structure.

In this paper, we investigate the anomaly/change-point detection
problem for time-series of attributed graphs. In the spirit of
time-series analysis, we approach this problem via test statistics
based on the moving average of some graph invariants. We derive
limiting distributions for these test statistics under a generative
model for the attributed graphs. The experimental evidence indicate that these
test statistics have sufficient power against the null hypothesis for
a general class of change-point problem but that many interesting
questions remain to ponder.

Our paper is structured as follows. We present an overview of the
generative model for time-series of attributed graphs \cite{lee11} in
\S\ref{sec:latent-process-model}. The change-point detection problem
along with the relevant test statistics are described in
\S\ref{sec:change-point-detect}. The distributions of the test
statistics and the power estimates for hypothesis testing using
these test statistics are given in \S\ref{sec:power-estimates}. 
\S\ref{sec:inference-examples} presents a
synthetic data example and an example using the Enron email corpus to
illustrate our methodology.
\section{Latent Process Model}
\label{sec:latent-process-model}
The abundance of graph representations in diverse application domains
leads to a proliferation of random graph models to model the
association among the entities. In addition to the classical
Erd\"{o}s-R\'{e}nyi model \cite{Erdos1959}, there is, the exponential
graph models \cite{holland81}, the stochastic block model
\cite{Holland1983,Wang1987}, the latent position model
\cite{hoff02:_laten}, the dot product model for unattributed graphs
\cite{young07:_random} and the latent process model for attributed
graphs \cite{lee11}. We now describe briefly the latent position model
of \cite{hoff02:_laten} and the dot product model of
\cite{young07:_random} as they are closely related to the latent
process model in \cite{lee11}. The latent process model serves as
the generative model for our attributed graphs and thus plays a
key role in our setup of the anomaly detection problem in 
\S~\ref{sec:change-point-detect}.

The latent space model is one where each vertex is associated with a
latent random vector. There may also be additional covariate
information but those are not relevant to the current exposition. The
vectors are independent and identically distributed and the
probability of an edge between two nodes depends only on their latent
vectors. Conditioned on the latent vectors, the presence of each edge
is an independent Bernoulli trial. For example, if $p_{uv}$ is the
probability of an edge between the vertices $u$ and $v$, then $p_{uv}
= \tfrac{1}{1 + \exp( - \|z_u - z_v\|)}$ where $z_u$ and $z_v$ are the
latent vectors associated with vertex $u$ and $v$, respectively.
Another example of a latent space model is the random dot product
graph model \cite{young07:_random}. Under the random dot product graph
model, the probability of an edge between two vertices is given
by the dot product of their respective latent vectors, i.e., $p_{uv} =
\langle z_u, z_v \rangle$. Parts of the motivation of the random dot
product graph model is that, in a social network with edges
indicating friendships, the components of the vector may be
interpreted as the relative interest of the individual in various
topics. The magnitude of the vector can be interpreted as how
talkative the individual is, with more talkative individuals more
likely to form relationships. Talkative individuals interested in the
same topics are most likely to form relationships while individuals
who do not share interests are unlikely to form relationships.

The latent process model for time-series of attributed graphs was
presented in \cite{lee11}. The main ideas underlying the model is as
follows. The model is motivated by the assumption that each vertex is
associated with a stochastic process, in this case a finite-state
continuous time Markov chain. The stochastic processes then associate
to each vertex $v$, for the time interval $(t-1,t]$, a (latent) random
vector $X_v(t)$ in the unit simplex with the $k$-th entry of $X_v(t)$
denoting the proportion of time the stochastic process spent in state
$k$. The probability of interactions between vertices $u$
and $v$ during $(t-1, t]$ is then given by the dot product of $X_u$
and $X_v$ a la the dot product model of \cite{young07:_random}, with a
slight modification to account for the presence of attributes on the
edges.

We now give the necessary details to make the above ideas precise. Let $G =
(V,E)$ be an undirected graph, equipped with an edge-attribution
function $\phi \colon \tbinom{V}{2} \mapsto [K+1] = \{1,2,\dots, K+1\}$ for
some $K$ such that $\phi(e) \leq K$ for $e \in E$ and $\phi(e) \equiv
K+1$ for $e \not \in E$. We refer to $\phi(e)$ as the attribute of
edge $e \in E$, and the pair $(G,\phi)$ as an attributed graph. Let
$\mathscr{S}$ be the unit simplex in $\mathbb{R}^{K}$, i.e.,
\begin{equation}
  \mathscr{S} = \{ \xi \in [0,1]^{K}
  \colon \sum_{k = 1}^{K} \xi_k \leq 1 \}.
\end{equation}
A {\em random dot product space} for attributed graphs with vertices
in $[n]$ ($[n] = \{1,2,\dots,n\}$) and edge attributes $[K]$ ($[K] =
\{1,2,\dots,K\}$) is a pair $(\mathbf{X},G)$ of
random elements such that
\begin{enumerate}
\item $\mathbf{X} = \{X_v\}_{v = 1}^{n}$ is a collection of
  independent $\mathscr{S}$-valued random vectors.
\item $G$ is a random undirected graph with vertex set $[n]$ such
  that the edges of $G$ are conditionally independent given
  $\mathbf{X}$ and that
  \begin{equation}
    \label{eq:1}
    \mathbb{P}(u \sim v \,|\, \mathbf{X}) = \mathbb{P}(u \sim v \, |
    \, X_u,X_v) = \langle x_u, x_v \rangle.
  \end{equation}
 Furthermore, we also have 
  \begin{equation}
    \label{eq:11}
    \mathbb{P}(\phi(\{u,v\}) = k \, | \, u \sim
    v) = \frac{X_{u,k} X_{v,k}}{\langle X_u, X_v \rangle}
  \end{equation}
  i.e., the pair $\{u,v\}$ is an edge in $G$ with attribute $k$ with
  probability $X_{u,k} X_{v,k}$.  
\end{enumerate}
We say that a c\'{a}dl\'{a}g (right continuous with left limit)
process $W \colon [0,\infty) \mapsto [K+1]^{n}$ induces the sequence
of random dot product spaces $\mathscr{V} = (\mathbf{X}(t), G(t))_{t =
1}^{\infty}$ if
\begin{enumerate}
\item Each $(\mathbf{X}(t), G(t))$ is a random dot product space with
  vertices $[n]$ and edge attributes $[K]$. Furthermore, for each $v
  \in [n]$, $k \in [K]$ and $t \in \mathbb{N}$, we have
  \begin{equation}
    \label{eq:54}
  X_{v,k}(t)  = \int_{t - 1}^{t}{ \mathbf{1}\{W_v(\omega) = k\}\, d\omega}
  \end{equation}
  where we denote $W_{v}(\omega)$ as the $v$-th component of $W(\omega)$ and
  $X_{v,k}$ as the $k$-th component of $X_v$.  
\item  For each $t \in \mathbb{N}$, we have
  \begin{equation}
    \label{eq:2}
    \mathbb{P}(G(t) = g \,|\, \mathscr{F}_{\leq t}) = \mathbb{P}(G(t)
    = g \,|\, \mathbf{X}(t))
  \end{equation}
where $\mathscr{F}_{\leq t}$ is the $\sigma$-field generated by $\{W(s)
  \colon s \leq t\}$.
\end{enumerate}
We will call any pair $(\mathscr{V}, W)$ that satisfies the above
properties a random dot process model.

The random dot process model can be interpreted as follows. During
each time interval $(t-1,t]$, a particular vertex is inclined to
communicate with other vertices on a variety of topics (the topics are
enumerated as the elements of $[K]$). As the inclination of a vertex
$v$ is governed by its underlying stochastic process $W_v$, we can
view the $\{W_v\}$ as encapsulating a notion of changes in the
inclination of the vertices over time. The propensity for
vertex $v$ to communicate about a particular topic $k$ is $X_{v,k}(t)$
and two vertices $u$ and $v$ are more likely to exchange communication
on topic $k$ if the propensity to communicate about topic $k$ is high
for both $u$ and $v$. Eq.~\eqref{eq:1} and Eq.~\eqref{eq:11} attempt
to capture these two assumptions.

In this paper, we are interested in the pairs $(\mathscr{V}, W)$
possessing the following properties:
\begin{enumerate}
\item For each $t \in \mathbb{N}$ and vertex $v \in [n]$, there exists
  a matrix $\mathbf{Q}^{(v)}(t)$ such that $W_v$, when restricted to
  the interval $[t, t+1)$, is a stationary, continuous-time Markov
  chain with state space $[K+1]$, intensity matrix
  ${\mathbf{Q}^{(v)}(t)}$, and stationary distribution
  $\pi^{(v)}(t)$. $W$ is thus a piece-wise stationary, continuous-time
  Markov chain with state space $[K+1]^{n}$ and intensity matrix
  $\otimes_{v=1}^{n}\mathbf{Q}^{(v)}(t)$.
\item There exists a $t^{*} \in \mathbb{N}$ and a $m < n$ such that,
  for some $\pi_0, \pi_1, \mathbf{Q}_0$ and $\mathbf{Q}_1$ we have
  \begin{enumerate}
  \item for $t < t^{*} - 1$,
    \begin{gather*}
      \pi^{(1)}(t) = \dots = \pi^{(n)}(t) = \pi_0 \\
      \mathbf{Q}^{(1)}(t) = \dots = \mathbf{Q}^{(n)}(t) = \mathbf{Q}_0,
    \end{gather*}
  \item  for $t \geq t^{*} - 1$,
    \begin{gather*}
      \pi^{(1)}((t)) = \dots = \pi^{(m)}(t) = \pi_1 \\
      \pi^{(m+1)}(t)  = \dots = \pi^{(n)}(t) = \pi_0 \\
      \mathbf{Q}^{(1)}(t)  = \dots = \mathbf{Q}^{(m)}(t) = \mathbf{Q}_1 \\
      \mathbf{Q}^{(m+1)}(t) = \dots = \mathbf{Q}^{(n)}(t) = \mathbf{Q}_0 
    \end{gather*}
  \end{enumerate}
\end{enumerate}
The above properties characterize a random dot process model with a
change-point phenomena for $(X(t),G(t))$ at time $t = t^{*}$. We chose
$\{1,2,\dots,m\}$ as the set of vertices with a change in the
stationary distribution and intensity matrix at time $t = t^{*} - 1$
for ease of discussion. Permutation of the vertex labels does not
affect our subsequent analysis. We will refer to $(t^{*}, m, \pi_0,
\pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$ as the change parameters.  Because
$W$ is completely determined by the change parameters, we often choose
to omit $W$ and only mention $\mathscr{V}$ when referring to random
dot process models with change-point phenomena. For a random dot
process model
$\mathscr{V}$ with change parameters $(t^{*}, m, \pi_0, \pi_1,
\mathbf{Q}_0, \mathbf{Q}_1)$, we can construct several approximations
to $\mathscr{V}$. Of particular interests are the following two
approximations.
\begin{definition}
  \label{def:1}
  Let $\mathscr{V}$ be a random dot process model (rdpm) with change
  parameters $(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0,
  \mathbf{Q}_1)$. The first order approximation $\bar{\mathscr{V}}$ of
  $\mathscr{V}$ is the sequence $\{(\bar{X}(t), \bar{G}(t)\}_{t =
    1}^{\infty}$ of {\em independent} random dot product spaces such
  that
 \begin{enumerate}
 \item for $t < t^{*} - 1$,
   \begin{equation}
     \label{eq:5}
     \bar{X}_{v}(t)  \equiv \hat{\pi}_0 \quad \text{for all $v \in [n]$}
   \end{equation}
 \item for $t \geq t^{*} - 1$
   \begin{gather*}
     \bar{X}_{v}(t) \equiv \hat{\pi}_1 \quad \text{for $v \leq m$} \\
     \bar{X}_{v}(t) \equiv \hat{\pi}_0 \quad \text{for $v > m$} 
   \end{gather*}
 \end{enumerate}
 where $\hat{\pi}_0$ and $\hat{\pi}_1$ are sub-probability vectors
 obtained by removing the last coordinate of $\pi_0$ and $\pi_1$. 
\end{definition}
The first approximation yields a sequence of independent random graphs
with independent edges. For $t \leq t^{*} - 1$, $G(t)$ is an
attributed instance of the Erd\"{o}s-Renyi random graphs. For $t \geq
t^{*}$, $G(t)$ is an attributed instance of the kidney-egg model
\cite{rukhin11} (see \figurename~\ref{fig:kidney-egg}), which in
itself is a special instance of the mixed membership stochastic
blockmodel \cite{airoldi08:_mixed}. The unattributed kidney-egg model
can be denoted as $\kappa(n,m,p,q,s)$ with the quantities $p$, $q$,
and $s$ representing
\begin{gather*}
p = \mathbb{P}(u \sim v \, | \, u \leq m, v \leq m) = \langle
\hat{\pi}_1, \hat{\pi}_1 \rangle \\ q =
\mathbb{P}(u \sim v \, | \, u \leq m, v > m) = \langle \hat{\pi}_1,
\hat{\pi}_0 \rangle \\ s = \mathbb{P}(u
\sim v \, | \, u > m, v > m) = \langle \hat{\pi}_0, \hat{\pi}_0
\rangle . 
\end{gather*}
\begin{figure}[!tp]
  \centering
  \includegraphics[width=5cm]{graphics/Fig2-SSP2011-hat1.pdf}
  \caption{An illustration of the kidney-egg model for $G(t^{*})$. The
    probability of an edge is $\langle \hat{\pi}_0, \hat{\pi}_0
    \rangle$ for the connections inside the kidney set (black), $\langle
    \hat{\pi}_1, \hat{\pi}_1 \rangle$ for the connections inside the egg set
    (red), and $\langle \hat{\pi}_0, \hat{\pi}_1 \rangle$ for the connections
    between the kidney and egg set.}
  \label{fig:kidney-egg}
\end{figure}
 \begin{definition}
  \label{def:2}
  Let $\mathscr{V}$ be a rdpm with change parameters $(t^{*}, m,
  \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. Define
  $\mathbf{Z}_0$ and $\mathbf{Z}_1$ by
  \begin{gather}
    \mathbf{Z}_0 = (\mathbf{1}\mathbf{\pi}_0^{T} -
     \mathbf{Q}_0)^{-1}(\mathbf{I} - \mathbf{1}\pi_0^{T}) \\
    \mathbf{Z}_1 = (\mathbf{1}\mathbf{\pi}_1^{T} -
     \mathbf{Q}_1)^{-1}(\mathbf{I} - \mathbf{1}\pi_1^{T}).
  \end{gather}
  $\mathbf{Z}_0$ and $\mathbf{Z}_1$ are the fundamental matrices
  for the continuous-time Markov chain on $[K+1]$ with intensity
  matrix $\mathbf{Q}_0$ and $\mathbf{Q}_1$ (see
  e.g. \cite[p. 55]{asmussen03:_applied_probab_queues}). Let
  $\Sigma_0$ and $\Sigma_1$ be given by
  \begin{gather*}
    \Sigma_0 = \mathrm{diag}(\pi_0) \mathbf{Z}_0 +
    \mathbf{Z}_0^{T}
    \mathrm{diag}(\pi_0) \\
    \Sigma_1 = \mathrm{diag}(\pi_1) \mathbf{Z}_1 +
    \mathbf{Z}_1^{T}
    \mathrm{diag}(\pi_1).
  \end{gather*}
  The second order approximation $\widehat{\mathscr{V}}$ of
  $\mathscr{V}$ is the sequence $\{\widehat{X}(t),
  \widehat{G}(t)\}_{t=1}^{\infty}$ where
  \begin{enumerate}
  \item For each $t$ and each $v \in [n]$, $\widehat{X}_v(t)$ is a
    random vector obtained by truncating a multivariate normal random
    vector $Z_{v}(t)$ to $\mathscr{S}$ with mean and covariance
    matrices given below.
  \item For $t < t^{*} - 1$,
    \begin{equation}
      \mathbb{E}[Z_v(t)] \equiv \hat{\pi}_0, \,\, 
  \mathrm{Var}[Z_v(t)] \equiv \widehat{\Sigma}_0 \quad \text{for all
    $v$}.
    \end{equation}
  \item For $t \geq t^{*} - 1$,  
    \begin{gather*}
      \mathbb{E}[Z_v(t)] \equiv \hat{\pi}_1, \,\, 
\mathrm{Var}[Z_v(t)] \equiv \widehat{\Sigma}_{1} \quad \text{for $v
  \leq m$} \\
  \mathbb{E}[Z_v(t)] \equiv \hat{\pi}_0, \,\, \mathrm{Var}[Z_v(t)] \equiv \widehat{\Sigma}_{0} \quad \text{for $v
        > m$}
    \end{gather*}
    where $\widehat{\Sigma}_0$ and $\widehat{\Sigma}_1$ are the $K
    \times K$ matrices
    obtained by removing the last row and column of $\Sigma_0$ and
    $\Sigma_1$, respectively. 
  \end{enumerate}
\end{definition}
A second-order approximation yields a sequence of independent latent
position graphs
\cite{marchette08:_predic,scheinerman10:_model,hoff02:_laten}.  

Suppose that we have a sequence $\{\mathscr{V}^{r} \colon r > 0 \}$ of
rdpm with vertices $[n]$ and attributes $[K]$ where for each $r > 0$,
$\mathscr{V}^{r}$ has change parameters $(t^{*}, m, \pi_0, \pi_1, r
\mathbf{Q}_0, r \mathbf{Q}_1)$. The parameter $r$ can be thought of as
the vertex process rate, i.e., the waiting time decreases
exponentially as $r$ increases. Let us now consider the sequence of
first approximations $\bar{\mathscr{V}}^{r}$ and second approximations
$\widehat{\mathscr{V}}^{r}$ of $\mathscr{V}^{r}$. We note that
$\bar{\mathscr{V}}^{r_1} = \bar{\mathscr{V}}^{r_2}$ for any $r_1, r_2
> 0$. Let us then denote by $\bar{\mathscr{V}}$ the (a.e.) unique
first order approximation of $\mathscr{V}^{r}$ for $r > 0$. If we
denote by $\widehat{\Sigma}_{0}^{(r)}$ and
$\widehat{\Sigma}_{1}^{(r)}$ the matrices $\widehat{\Sigma}_0$ and
$\widehat{\Sigma}_1$ for $\widehat{\mathscr{V}}^{r}$, then
$\widehat{\Sigma}_0^{(r)} = \widehat{\Sigma}_0^{(1)}/r$ and
$\widehat{\Sigma}_{1}^{(r)} = \widehat{\Sigma}_1^{(1)}/r$. Therefore,
as $r \rightarrow \infty$, $\widehat{\Sigma}_0^{(r)} \rightarrow
\bm{0}$ and $\widehat{\Sigma}_1^{(r)} \rightarrow \bm{0}$ and so
$\widehat{\mathscr{V}}^{r} \overset{\mathrm{d}}{\longrightarrow}
\bar{\mathscr{V}}$. This indicates that for sufficiently large $r$,
there is little statistical difference among the
$\bar{\mathscr{V}}$, $\mathscr{V}^{r}$ and $\widehat{\mathscr{V}}^{r}$
(\cite[Theorem 2]{lee11}). The behavior of
$\widehat{\mathscr{V}}^{r}$ for ``small'' $r$ is less
clear. $\bar{\mathscr{V}}$ will thus serve as the basis for our
subsequent analysis on graph invariants for attributed random
graphs.

A comment should also be made about the utility of the exact
generative model. It is computationally intractable to estimate the
parameters of the underlying continuous time Markov chains, i.e. the
$\{\mathbf{Q}^{(v)}(t)\}$, given a snapshot of the graphs. Therefore,
the main interest of \cite{lee11}, and consequently of this paper,
lies not in the exact generative model but in its first and
second-order approximation. We will not be concerned with the
estimation of the latent position vectors for the first and
second-order approximation in this paper, but we note that they can be
done via spectral embedding technique a la \cite{STFP-2011}. We also
note that more refined models, where the messaging events between each pairs of
vertices is modeled as point process,
e.g. \cite{Heard2010,PerryWolfe2010}, can also be formulated. The
snapshot of the graphs is then a binning of these point processes and
the issue of parameters estimation is fundamental but feasible for
these point processes approaches.
\section{Change-point detection}
\label{sec:change-point-detect}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig1-SSP2011.pdf}
  \caption{Notional depiction of the problem of change-point detection
    in a time-series of graphs}
  \label{fig:notional_change_point}
\end{figure}
Let $\mathscr{V}$ be a random dot process model with change parameters
$(t^{*}, m, \pi_0, \pi_1, \mathbf{Q}_0, \mathbf{Q}_1)$. The
change-parameters encapsulate a notion of chatter anomalies, i.e., a
subset of vertices of $\mathscr{V}$ with altered communication
behavior in an otherwise stationary setting as depicted in
Fig.~\ref{fig:notional_change_point}. We are interested in the problem
of testing, for a $t \in \mathbb{N}$, the hypotheses that $t$ is the
change-point of $\mathscr{V}$, namely
\begin{gather*}
  \mathscr{H}_0 \colon t^{*} > t \\
  \mathscr{H}_A \colon t^{*} = t
\end{gather*}
This will be done using the notion of fusion of attributed graph
invariants. The particular invariants of interests are the size
$\mathcal{E}$, number of triangles $\tau$, scan $\Psi$, and max degree
$\Delta$. The scan statistics $\Psi$ are introduced and applied to the
problem of detecting chatter anomalies in
\cite{priebe05:_scan_statis_enron_graph}. The use of various graph
invariants as test statistics was considered in
\cite{pao11:_statis_infer_random_graph} and it was shown there, via
Monte Carlo analysis, that no single invariant is uniformly most
powerful.

Let $(G,\phi)$ be an attributed graph. For a given $\{u,v\}
\in \tbinom{V}{2}$, we define a $\Gamma_{uv} \in \mathbb{R}^{K}$ by
$\Gamma_{uv}(k) = \mathbb{I}\{\phi(\{u,v\}) = k\}$. Thus $\Gamma_{uv}
= \bm{0}$ unless $\{u,v\} \in E$. Under the independent edges
assumption, the $\Gamma_{uv}$ are independent. We consider linear
attribute fusion of graph invariants with parameter $\lambda \in
\mathbb{R}^{K}$ via
%\begin{gather}
%  \label{eq:6}
%  \mathcal{E}_{\lambda}(G) = \sum_{k=1}^{K} \lambda_k \sum_{u,v}
%  \mathbb{I}\{ \phi(uv) = k \} \\
%  \tau_{\lambda}(G) = \sum_{(i,j,k) \in K^{3}\vphantom{\tbinom{V}{3}}}
%  \, \, \sum_{(u,v,w)
%      \in \tbinom{V}{3}} \lambda_{i}\lambda_j \lambda_k h_{ijk}(u,v,w) \\
%  \Delta_{\lambda}(G) = \max_{v \in V} \, \sum_{k = 1}^{K} \lambda_k
%  \sum_{u \in N(v)}{\mathbb{I}\{\phi(\{u,v\}) = k\}} \\
%  \label{eq:3}
%  \Psi_{\lambda}(G) = \max_{v \in V} \sum_{k = 1}^{K}
%  \lambda_k \!\!\sum_{u,w \in N(v)}\!\!\! \mathbb{I}\{\phi(\{u,w\}) = k\} 
%  \end{gather}

\begin{align}
  \label{eq:6}
  \mathcal{E}_{\lambda}(G) &= \sum_{\mathclap{\{u,v\} \in
      \tbinom{V}{2}}} \langle
  \lambda, \Gamma_{uv} \rangle \\
  \tau_{\lambda}(G) &= \sum_{\mathclap{\{u,v,w\} \in \tbinom{V}{3}}} \langle
  \lambda, \Gamma_{uv} \rangle \langle \lambda,
  \Gamma_{uw} \rangle \langle \lambda, \Gamma_{vw}
  \rangle \\ 
\Delta_{\lambda}(G) &= \max_{v \in V} \, \sum_{w \in N(v)} \langle
\lambda, \Gamma_{vw} \rangle \\
\label{eq:3}
  \Psi_{\lambda}(G) &= \max_{v \in V} \sum_{u,w \in N[v]} \langle
  \lambda, \Gamma_{uw} \rangle.
  \end{align}
  where $N(v) = \{u \colon u \sim v\}$ is the set of neighbors of
  $v$ and $N[v] = v \cup N(v)$.  

  Let $\{G(t)\}$ be a time series of graphs. Let $J_{\lambda}(t)$ be a
  statistic for $G(t)$ of the form as in Eq.~\eqref{eq:6} through
  Eq.~\eqref{eq:3}. We define the running average 
  $\bar{J}^{l}_\lambda(t)$ of $J_{\lambda}$ as
\begin{equation}
  \label{eq:4}
 \bar{J}^{l}_{\lambda}(t) = \frac{1}{l}\sum_{s = 1}^{l} J_{\lambda}(t - s) 
\end{equation}
where $l \in \mathbb{N}$ specified the width of the running-average
window. Our main interest is in the normalized fusion statistic
$T_{\lambda}^{l}(t)$ as depicted in Fig.~\ref{fig:temporal}, namely
\begin{equation}
  \label{eq:7}
 T_{\lambda}^{l}(t) = % \begin{cases}
   % J_{\lambda}(t) - J_{\lambda}(t - 1) & \text{if $m = 1$} \\
   \frac{J_{\lambda}(t) -
     \bar{J}_{\lambda}^{l}(t)}{\sqrt{\tfrac{1}{l-1}
       \sum_{s=1}^{l}(J_{\lambda}(t - s) - \bar{J}_{\lambda}^{l}(t))^2}}
%   & \text{if $m \geq 2$}
 %  \end{cases}
\end{equation}
for $l \geq 2$, i.e., we want to find a parameter $\lambda$ such that
the power of the test using $T_{\lambda}^{l}$ is maximized. We note that
$T_{\lambda}^{l}$ is scale invariant in $\lambda$ for all of our graph
invariants. It was noted in \cite{lee11} that the maximum of the power
for scale invariant test statistics is attainable in the set $\{ \lambda \colon \|
\lambda \| = 1 \}$. 

As we have mentioned earlier, we will assume that the number of
vertices $n$ is large and that the use of the first order
approximation $\bar{\mathscr{V}}$ of $\mathscr{V}$ is appropriate in
our study of the asymptotic theory for these graphs invariants. We
strive to obtain approximations for the power in testing $t^{*} > t$
against $t^{*} = t$ using $T_{\lambda}^{l}(t)$ and from these
approximations, find the $\lambda$ that maximize the power. We now
introduce additional notation that will be used later on in the
paper. First, let $\pi_{00}, \pi_{01}$, and $\pi_{11}$ be
sub-probability vectors whose components are given by
\begin{gather*}
  \pi_{00}(k) = \pi_{0}(k) \pi_{0}(k) \\
  \pi_{01}(k) = \pi_{0}(k) \pi_{1}(k) \\
  \pi_{11}(k) = \pi_{1}(k) \pi_{1}(k)
\end{gather*}
where $k \in [K]$. We also let $\eta_{00}, \eta_{01}$, and
$\eta_{11}$ be matrices of size $K \times K$ defined by 
\begin{gather*}
  \eta_{00} = \mathrm{diag}(\pi_{00}) - \pi_{00} \pi_{00}^{T} \\
  \eta_{01} = \mathrm{diag}(\pi_{01}) - \pi_{01} \pi_{01}^{T} \\
  \eta_{11} = \mathrm{diag}(\pi_{11}) - \pi_{11} \pi_{11}^{T}.
\end{gather*}
% The structure of the remaining sections of the paper is as follows. We
% discussed the power estimates for $\mathcal{E}_{\lambda},
% \tau_{\lambda}, \Delta_{\lambda}$ and $\Psi_{\lambda}$ in
% \S~\ref{sec:power-estim-mathc}
%  through \S~\ref{sec:power-estim-psi_l}. We illustrate our analysis with some
% simulation experiments in \S~\ref{sec:experimental-results}  
\begin{figure}[htbp]
  \centering
  \includegraphics[width=8cm]{graphics/Fig4-SSP2011.pdf}
  \caption{Temporal standardization: when testing for change at time
    $t$, the recent past (graphs $G(t - l), \dots, G(t-1))$ is used to
    standardize the invariants}
  \label{fig:temporal}
\end{figure}
\section{Power estimates}
\label{sec:power-estimates}
\subsection{Power estimates for $\mathcal{E}_\lambda$}
\label{sec:power-estim-mathc}
It was shown in \cite{lee11} that if $J_\lambda(t) =
\mathcal{E}_{\lambda}(G(t))$, then $T_{\lambda}^{l}(t)$ follows a
$t$-distribution with $l - 1$ degrees of freedom in the
limit. Specifically, we have the following results
\begin{theorem}
  \label{thm:9}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. Define the vector $\zeta$ and the matrix $\xi$ by
  \begin{gather*}
    \zeta = \tbinom{m}{2}(\pi_{11} - \pi_{00}) + (n-m)m(\pi_{01} -
    \pi_{00}) \\
    \xi = \tfrac{l+1}{l}\tbinom{n}{2} \eta_{00} +
          \tbinom{m}{2}(\eta_{11} - \eta_{00}) + (n-m)m(\eta_{01} -
          \eta_{00}).
  \end{gather*}
  Define the random variable $\psi^{l}_{\lambda}(t)$ by
  \begin{equation}
    \label{eq:10}
    \psi^{l}_{\lambda}(t) = \begin{cases}
      \sqrt{\frac{l}{l + 1}} T_{\lambda}^{l}(t)& \text{if $t < t^{*}$}
      \\ \sqrt{\frac{\langle \lambda, \tbinom{n}{2} \eta_{00}
            \lambda \rangle}{\langle \lambda, \xi
            \lambda \rangle}} T_{\lambda}^{l}(t) & \text{if $t =
          t^{*}$}.
      \end{cases}
  \end{equation}
As $n \rightarrow \infty$,
  $\psi^{l}_{\lambda}(t)$ converges weakly to the Student
  $t$-distribution with $l-1$ degrees of freedom and non-centrality
  parameter $\mu_{\lambda}$, where
  \begin{equation}
    \label{eq:15}
    \mu_{\lambda} = \begin{cases}
      0 & \text{if $t < t^{*}$} \\
      \frac{\langle \lambda, \zeta \rangle}{\sqrt{\langle \lambda, \xi \lambda \rangle}} & \text{if $t = t^{*}$} 
    \end{cases}
  \end{equation}
\end{theorem}
We are interested in finding the $\lambda$ that will maximize the
power of the test. The power approximation is determined by various
factors but the dominating factor is $\mu_{\lambda}$ (for large $n$). 
The following corollary extends a result in
\cite{lee11} for $K = 2$ to the case
where $K \geq 2$. 
\begin{corollary}
  \label{cor:1}
  Let $\zeta$ and $\xi$ be as defined in Theorem~\ref{thm:9}. Suppose
  that $\xi$ is also positive definite. Let $\nu^{*}$ be the
  normalized eigenvector corresponding to the largest eigenvalue of $ \xi^{-1/2}
  \zeta \zeta^{T} \xi^{-1/2}$, i.e.,
 \begin{equation}
   \label{eq:9}
  \nu^{*} = \argmax_{\nu \colon \| \nu \| = 1}
  \nu^{T} \xi^{-1/2} \zeta \zeta^{T} \xi^{-1/2}
  \nu.
 \end{equation}
 Then $\lambda^{*} = \tfrac{\xi^{-1/2} \nu^{*}}{\|\xi^{-1/2} \nu^{*} \|}$ satisfies
 \begin{gather}
   \label{eq:8}
 \argmax_{ \|\lambda\| = 1}
 \mu_{\lambda} \, \cap \, \{\lambda^{*}, - \lambda^{*}\} \not = \emptyset \\
 \argmin_{ \| \lambda \| = 1} \mu_{\lambda} \, \cap \, \{\lambda^{*}, -
 \lambda^{*}\} \not = \emptyset.
 \end{gather}
\end{corollary}
\subsection{Power estimates for $\tau_{\lambda}$}
\label{sec:power-estim-tau_l}
The limiting distribution for the number of triangles in unattributed
random graphs was considered in
\cite{nowicki88:_subgr_u_statis_method} for the Erd\"{o}s-Renyi and in
\cite{rukhin09:_asymp_analy_various_statis_random_graph_infer} for the
kidney-egg model. We note here the small changes that allow us to
extend the results in
\cite{rukhin09:_asymp_analy_various_statis_random_graph_infer,%
nowicki88:_subgr_u_statis_method} to our attributed graphs model. Let
$Y_{uv} = \langle \lambda, \Gamma_{uv} \rangle$ for $\{u,v\} \in
\tbinom{V}{2}$. We can now write $\tau_{\lambda}(G(t))$ as
\begin{equation}
  \label{eq:45}
  \frac{\tau_{\lambda}(G(t))}{\tbinom{n}{3}} = \tbinom{n}{3}^{-1}
  \sum_{\mathclap{\{u,v,w\} \in \tbinom{V}{3}}} Y_{uv} Y_{uw} Y_{vw};
\end{equation}
$\tau_{\lambda}(G(t))/\tbinom{n}{3}$ is then an
$U$-statistic on $\{Y_{e}\}$, with kernel function $h(Y_{1}, Y_{2},
Y_{3}) = Y_{1} Y_{2} Y_{3}$. By using Hajek's projection method, we
can show that $\tau_{\lambda}(G(t))$ converges to a normal
distribution as $n \rightarrow \infty$. 
\begin{lemma}
  \label{lem:3}
  Let $\tau_{\lambda}^{*}$ be the Hajek's projection of $\tau_{\lambda}$, i.e.,
\begin{equation}
  \label{eq:46}
  \tau_{\lambda}^{*}(G) - \mathbb{E}[\tau_{\lambda}(G)] =
  \sum_{\mathclap{\{u,v\} \in \tbinom{V}{2}}} \, \Bigl(\mathbb{E}[
  \tau_{\lambda}(G) \, \lvert \, Y_{uv}] -
  \mathbb{E}[\tau_{\lambda}(G)]\Bigr).
\end{equation}
For $t < t^{*}$, we have
\begin{align*}
  \mathbb{E}[\tau_{\lambda}^{*}(G(t))] &= \tbinom{n}{3} \langle 
  \lambda, \pi_{00} \rangle^{3} \\
  \sqrt{\mathrm{Var}[\tau_{\lambda}^{*}(G(t))]} &= (n-2) \langle \lambda,
  \pi_{00} \rangle^{2} \sqrt{\tbinom{n}{2} \langle \lambda, \eta_{00}
  \lambda \rangle}.
\end{align*}
For $t = t^{*}$, we have
  \begin{gather*}
    \label{eq:42}
    \begin{split}
    \mathbb{E}[\tau_{\lambda}^{*}(G(t))] = \tbinom{m}{3}
    \langle \lambda, \pi_{11} \rangle^{3} &+
    \tbinom{m}{2}(n-m) \langle \lambda, \pi_{11} \rangle \langle
    \lambda, \pi_{01} \rangle^{2} \\ &+ m \tbinom{n-m}{2} \langle \lambda,
    \pi_{01} \rangle^{2} \langle \lambda, \pi_{00} \rangle \\ &+
    \tbinom{n-m}{3} \langle \lambda, \pi_{00} \rangle^{3}
    \end{split} \\
    \begin{split}
    \mathrm{Var}[\tau_{\lambda}^{*}(G(t))] = \tbinom{m}{2} \langle \lambda, \eta_{11} \lambda
    \rangle S_1^2  
    &+ m(n-m) \langle \lambda, \eta_{01} \lambda \rangle S_2^2  \\
    &+ \tbinom{n-m}{2} \langle \lambda, \eta_{00} \lambda \rangle
    S_3^2
    \end{split}
  \end{gather*}
  where $S_1$, $S_2$, and $S_3$ are given by
\begin{align*}
    S_1 &= (m-2) \langle \lambda, \pi_{11} \rangle^{2} + (n-m)
    \langle \lambda, \pi_{01} \rangle^{2} \\ 
    S_2 &= (m-1)
    \langle \lambda, \pi_{11} \rangle \langle \lambda, \pi_{01}
    \rangle + (n-m-1) \langle \lambda, \pi_{00} \rangle \langle
    \lambda, \pi_{01} \rangle \\
    S_3 &= m \langle \lambda, \pi_{01} \rangle^{2} + (n-m-2) \langle
    \lambda, \pi_{00} \rangle^{2}.
    \end{align*}
  As $n \rightarrow \infty$, we have
  \begin{equation}
    \label{eq:49}
    \frac{\tau_{\lambda}(t) -
      \mathbb{E}[\tau_{\lambda}^{*}(G(t))]}{\sqrt{\mathrm{Var}[\tau_{\lambda}^{*}(G(t))]}}
\overset{\mathrm{d}}{\longrightarrow}  \mathcal{N}(0,1).
  \end{equation}
\end{lemma}
From Lemma~\ref{lem:3}, we can show that the limiting distribution of
$T_{\lambda}^{l}(t)$ is once again a Student t-distribution with
$l-1$ degrees of freedom.
\begin{theorem}
  \label{thm:5}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. Let $\mu_0 = \mathbb{E}[\tau_{\lambda}^{*}(G(t))]$
  and $\sigma_0^{2} = \mathrm{Var}[\tau_{\lambda}^{*}(G(t))]$ for $t <
  t^{*}$. Let $\mu_A = \mathbb{E}[\tau_{\lambda}^{*}(G(t^{*}))]$ and
  $\sigma_{A}^{2} = \mathrm{Var}[\tau_{\lambda}^{*}(G(t^{*}))]$. Define
  the random variable $\psi_{\lambda}^{l}(t)$ by
  \begin{equation}
    \label{eq:50}
    \psi_{\lambda}^{l}(t) = \begin{cases}
      \sqrt{\tfrac{l}{l+1}}\, T_{\lambda}^{l}(t) & \text{if $t <
        t^{*}$} \\
      \sqrt{\tfrac{l \sigma_0^{2}}{l \sigma_A^{2} + \sigma_0^{2}} }\,
      T_{\lambda}^{l}(t) & \text{if $t = t^{*}$}.
        \end{cases}
  \end{equation}
  As $n \rightarrow \infty$, $\psi_{\lambda}^{l}(t)$ converges to a Student $t$-distribution with $l - 1$ degrees of freedom and
  non-centrality parameter $\mu_{\lambda}$ where
  \begin{equation}
    \label{eq:51}
    \mu_{\lambda} = \begin{cases} 0 & \text{if $t < t^{*}$} \\
      \frac{\mu_A - \mu_0}{\sqrt{l \sigma_A^{2} + \sigma_0^{2}}} &
       \text{if $t = t^{*}$}.
      \end{cases}
  \end{equation}
\end{theorem}
The power approximation for $\tau_{\lambda}$ is determined by various
factors, but similar to the power approximation for
$\mathcal{E}_{\lambda}$, the dominating factor is $\mu_\lambda$ (for
large $n$). Thus, we are also interested in finding the $\lambda$
that will maximize $\mu_\lambda$. However, because of the high power
of $\lambda$ in the expression for $\mu_{\lambda}$, an exact 
solution to $\argmax_{\lambda}{\mu_{\lambda}}$ is more challenging for
$\tau$ than for $\mathcal{E}$. 
\subsection{Power estimates for $\Delta_{\lambda}(t)$}
\label{sec:power-estim-delt}
The limiting distribution for maximum degree in unattributed random
graphs was considered in \cite{bollobas85:_random_graph} for the
Erd\"{o}s-Renyi and in \cite{rukhin11} for the kidney-egg model. We
note here the necessary changes that allow us to to extend the results
in \cite{bollobas85:_random_graph,rukhin11} to our attributed graphs
model. We denote by $\mathcal{G}(\alpha,\beta)$ the Gumbel
distribution with location parameter $\alpha$ and scale parameter
$\beta$.
\begin{proposition}
  \label{prop:3}
  Let $a_n$ and $b_n$ be functions of $n$ given by
  \begin{align*}
    a_n &= (2 \log{n})^{1/2}\Bigl(1 - \frac{\log{\log{n}} + \log{4\pi}}{4 \log{n}} \Bigr) \\ 
    b_n &= (2 \log{n})^{-1/2}.
  \end{align*}
  Then as $n \rightarrow \infty$ and $m = \Omega( \sqrt{n \log{n}})$,
  we have
  \begin{gather}
    \label{eq:14}
    \frac{\Delta_{\lambda}(t) - a_{n} \sigma_0 - \mu_0}{b_n \sigma_0}
   \overset{\mathrm{d}}{\longrightarrow}  \mathcal{G}(0,1) \quad \text{for $t < t^{*}$ } \\
    \frac{\Delta_{\lambda}(t) - a_{m} \sigma_A - \mu_A}{b_m \sigma_A}
       \overset{\mathrm{d}}{\longrightarrow}\mathcal{G}(0,1) \quad \text{for $t = t^{*}$ }
  \end{gather}
  where
  \begin{align*}
    \mu_0 &= (n-1)\langle \lambda, \pi_{00} \rangle \\
    \sigma_A &= \sqrt{(n-1)\langle \lambda, \eta_{00} \lambda \rangle} \\
    \mu_0 &= \,\, (m - 1) \langle \lambda, \pi_{11} \rangle + (n-
    m)\langle \lambda, \pi_{01} \rangle
    \\ \sigma_A &= \sqrt{ (m - 1) \langle \lambda, \eta_{11} \lambda \rangle + (n -
      m) \langle \lambda, \eta_{01} \lambda \rangle}.
    \end{align*}
\end{proposition}
$T_{\lambda}^{l}(t)$ based on the Gumbel distributed
$\Delta_{\lambda}(t)$, in contrast to $T_{\lambda}^{l}(t)$ based on
the normally distributed $\mathcal{E}_{\lambda}$ and
$\tau_{\lambda}(t)$, does not have a simple distribution for small or
moderate values of $l$. The following result give the limiting
distribution for $T_{\lambda}^{l}(t)$ under the assumption that $l$ is
sufficiently large. 
\begin{theorem}
  \label{thm:8}
  Let $t \in \{l+1, \dots, t^{*}\}$ and $\lambda \in
  \mathbb{R}^{K}$. For sufficiently large $n$ and sufficiently large
  $l$, the variable $T_{\lambda}^{l}(t)$ has approximately a
  $\mathcal{G}(\rho_{\lambda}, \varsigma_{\lambda})$ distribution with
\begin{align}
  \label{eq:13}
  \rho_{\lambda} &= \begin{cases}
      - \frac{ \sqrt{\pi}}{6} \gamma & \text{if $t < t^{*}$} \\
    \frac{\sqrt{\pi}}{6} \frac{\mu_A - \mu_0 + a_m\sigma_A - (a_n + b_n
      \gamma)\sigma_0}{b_n \sigma_0} & \text{if $t =
        t^{*}$} 
  \end{cases}\\
  \varsigma_{\lambda} &= \begin{cases}
    \frac{\sqrt{\pi}}{6} & \text{if $t < t^{*}$} \\
    \frac{\sqrt{\pi} b_m \sigma_A}{6 b_n \sigma_0} & \text{if $t =
      t^{*}$} \\
    \end{cases}
\end{align}
where $\gamma \approx 0.57721$ is the Euler-Mascheroni constant.
\end{theorem}
We are interested in finding the $\lambda$ that will maximize the
power of the test using $\Delta_{\lambda}$. The power approximation is
determined by various factors, but the dominating factor is
$\tfrac{\rho_\lambda}{\varsigma_\lambda}$. For sufficiently large $n$
and $m$, we have for $t = t^{*}$,
\begin{equation*}
  \begin{split}
  \frac{\rho_\lambda}{\varsigma_\lambda} &= (1 + o(1)) \frac{\mu_A - \mu_0}{b_m
    \sigma_A} \\ &= (1 + o(1)) \frac{\langle \lambda, (m-1)\pi_{11} + (n-m)\pi_{01} -
    (n-1) \pi_{00} \rangle}{b_m \sqrt{\langle \lambda, ((m-1) \eta_{11} +
    (n-m)\eta_{01}) \lambda \rangle}}.
  \end{split}
\end{equation*}
We can thus find the maximum and minimum of
$\tfrac{\rho_{\lambda}}{\varsigma_{\lambda}}$ by solving an 
eigenvalue problem as in Corollary~\ref{cor:1}.
\subsection{Power estimates for $\Psi_{\lambda}(t)$}
\label{sec:power-estim-psi_l}
The limiting distribution for the scan statistics in unattributed
random graphs was considered in
\cite{rukhin12}. We note here the changes
that allow us to extend the results in
\cite{rukhin12} to our attributed graphs
model. Let $z_n$ be defined, for $n \in \mathbb{N}$, by
\begin{equation}
  \label{eq:20}
   z_n = \sqrt{2
      \log{n}}\Bigl(1 - \frac{\log{\log{n}} + \log{4 \pi}}{4
      \log{n}}\Bigr).
\end{equation}
Let us also define two random variables $E$ and $F$ by
\begin{gather*}
 %  B \sim \mathrm{Bin}(m, \langle 1, \pi_{00} \rangle); \quad C \sim \mathrm{Bin}(n-m-1,
 % \langle 1, \pi_{01} \rangle) \\
  E \sim \mathrm{Bin}(m-1, \langle 1, \pi_{11} \rangle) \\ F \sim \mathrm{Bin}(n-m,
  \langle 1, \pi_{01} \rangle).
\end{gather*}%
Let $E + F$ be the convolution of $E$ and $F$. Denote by $\mu_{E+F}$ and
$\sigma_{E+F}^{2}$ the mean and variance of $E + F$. Let
$N_{\kappa} = \mu_{E + F} + z_m \sigma_{E + F}$. We then have the
following results.
\begin{lemma}
  \label{lem:5}
  Let $\Psi_{\lambda}(t)$ be the scan statistic for $t < t^{*}$. Let
  $N_{0}$, $a_n$, and $b_n$ be defined by
\begin{align*}
   \mu_0 &= (n-1) \langle 1, \pi_{00} \rangle \\
   \sigma_0 &= \sqrt{(n-1) (\langle 1, \pi_{00} \rangle - \langle
     1, \pi_{00} \rangle^2)} \\ 
    N_{0} &= \mu_0 + z_n \sigma_0 \\
  %  a_n &= N_{0} \frac{\langle \lambda, \pi_{00} \rangle}{\langle
  %    1, \pi_{00} \rangle} + C_{00}p_{00}\tbinom{N_{0}}{2} \\ 
    a_n &=  \langle \lambda, \pi_{00} \rangle \tbinom{N_{0}}{2} \\ 
   b_n &= \langle \lambda, \pi_{00} \rangle N_{0} \frac{\sigma_0}{\sqrt{2 \log
     n}}.
%   b_n &= (1 - \tfrac{C_{00}p_{00}}{2} + C_{00}p_{00} N_{0}) \frac{\sigma_0}{\sqrt{2 \log
%     n}}
\end{align*}
Then we have
\begin{equation}
  \frac{\Psi_{\lambda}(t) - a_{n}}{b_n}
  \overset{\mathrm{d}}{\longrightarrow}  \mathcal{G}(0, 1).
\end{equation}
\end{lemma}%
\begin{lemma}
  \label{lem:6}
  Let $a_{n,m}$ and $b_{n,m}$ be given by
  \begin{gather}
    \label{eq:28}
    \begin{split}
    a_{n,m} =%  N_{\kappa} \frac{\langle x, \pi_{01} \rangle}{\langle
 %      1, \pi_{01} \rangle} \\
 % & + 
\langle \lambda, \pi_{00} \rangle \tbinom{N_\kappa}{2} &+
\langle \lambda, \pi_{11} - \pi_{00} \rangle
\tbinom{\mu_E}{2} \\ &+ \langle \lambda, \pi_{01} - \pi_{00} \rangle \mu_E \mu_F 
    \end{split} \\
    b_{n,m} = \langle \lambda, \pi_{00} \rangle N_\kappa \frac{\sigma_{E + F}}{\sqrt{2
        \log{m}}}.
    %b_{n,m} = (1 - \tfrac{C_{00} p_{00}}{2} + C_{00} p_{00}
    %N_\kappa)\frac{\sigma_{E + F}}{\sqrt{2 \log{n_1}}}
  \end{gather}
  If $m = \Omega(\sqrt{n \log n})$ and $m = O(n^{k/(k+1)})$ for some
  $k \in \mathbb{N}$ then
  \begin{equation}
    \label{eq:29}
    \frac{\Psi_{\lambda}(t^{*}) - a_{n,m}}{b_{n,m}}
    \overset{\mathrm{d}}{\longrightarrow} \mathcal{G}(0,1).
  \end{equation}
\end{lemma}
The next result is analogous to Theorem \ref{thm:8} and gives 
the limiting distribution for
$T_{\lambda}^{l}(t)$ based on $\Psi_{\lambda}$ for the case of large
$l$.
\begin{theorem}
  \label{thm:6}
  Let $t \in \{l+1, \dots, t^{*}\} $ and $\lambda \in
  \mathbb{R}^{K}$. For sufficiently large $n$ and sufficiently large
  $l$, $T_{\lambda}^{l}(t)$ has approximately a
  $\mathcal{G}(\rho_{\lambda}, \varsigma_{l})$ distribution with
  \begin{align}
    \label{eq:52}
    \rho_{\lambda} &= \begin{cases}
      - \tfrac{\sqrt{\pi}}{6} \gamma & \text{if $t < t^{*}$} \\
      \tfrac{\sqrt{\pi}}{6} \tfrac{a_{n,m} - a_n - b_n \gamma}{b_n} & \text{if
        $t = t^{*}$} 
    \end{cases} \\
      \varsigma_{\lambda} &= \begin{cases}
        \tfrac{\sqrt{\pi}}{6} & \text{if $t < t^{*}$} \\
        \tfrac{\sqrt{\pi} b_{n,m}}{6 b_n} & \text{if $t = t^{*}$}. \\
      \end{cases}
  \end{align}
\end{theorem}
The dominating factor in the power approximation for $\Psi_{\lambda}$
is $\tfrac{\rho_{\lambda}}{\varsigma_{\lambda}}$. For $t = t^{*}$ and
sufficiently large $n$ and $l$, we have
\begin{equation}
  \label{eq:53}
  \begin{split}
    \frac{\rho_{\lambda}}{\varsigma_{l}} &= \frac{a_{n,m} - a_n - b_n
      \gamma}{b_{n,m}} \\
    &= (1 + o(1)) \frac{ \langle \lambda, \xi \rangle}{\langle
      \lambda, N_{\kappa} \tfrac{\sigma_{E+F}}{\sqrt{2 \log{n}}}
      \pi_{00} \rangle}
  \end{split}
\end{equation}
where $\xi$ is given by
\begin{equation*}
 \xi =  \Bigl(\tbinom{N_{\kappa}}{2} -
    \tbinom{N_0}{2} - \tbinom{\mu_E}{2} - \mu_E \mu_F\Bigr)\pi_{00} +
    \tbinom{\mu_{E}}{2} \pi_{11} + \mu_E \mu_F \pi_{10}.
\end{equation*}
From Eq.~\eqref{eq:53}, we see that there exists (as $n \rightarrow
\infty$) a $\lambda$ that maximizes
$\tfrac{\rho_\lambda}{\varsigma_{\lambda}}$ and is on the boundary,
i.e., $\lambda_k \not = 0$ for exactly one $k$. Therefore the asymptotic theory
for scan statistics, in contrast with the other graph invariants that
were considered, indicates that there may be no benefits in fusing
attributes. However, as \figurename~\ref{fig5:subfig_scan} shows, this
phenomenon does not hold in general for moderate values of
$n$. Similar observations about potential inaccuracies in using
asymptotic results to predict finite but large samples behavior
in testing using graph invariants were discussed in
\cite{rukhin11,priebe10:_you_i}.   
%\section{Experimental Results}
%\label{sec:experimental-results}
\section{Inference Examples}
\label{sec:inference-examples}
We present here two inference examples, one on simulated data and the
other on the Enron email data set. In the first inference example, we
simulated data from the model in \S~\ref{sec:latent-process-model}
with the following parameters:
\begin{equation*}
  K = 2, n = 100, m = 9, l = 10,
\end{equation*}
transition matrices
\begin{equation*}
  \label{eq:55}
  \begin{matrix}
  \mathbf{Q}_0 = \begin{bmatrix}
    -\tfrac{2}{3} & \tfrac{1}{6} & \tfrac{1}{2} \\
    1 & -1 & 0 \\
    -1 & 0 & -1 
  \end{bmatrix},
  & \mathbf{Q}_1 = \begin{bmatrix}
    -\tfrac{13}{7} & \tfrac{5}{7} & \tfrac{8}{7} \\
    1 & -1 & 0 \\
    1 & 0 & -1
  \end{bmatrix}
  \end{matrix},
\end{equation*}
and stationary probability vectors
\begin{equation*}
  \pi_0 = (0.10,0.30,0.60)^{T}, \quad \pi_1 = (0.25,0.40,0.35)^{T}.
\end{equation*}
The change-point $t^{*}$ for this example is $t^{*} = 11$. 
Power estimates for our attribute fusion statistics for this example
are presented in \figurename~\ref{fig:power-scale} and
\figurename~\ref{fig:power-estimate}. We consider the asymptotic,
first approximation, second approximation, and exact model power
estimates. \figurename~\ref{fig:power-scale} shows power as a function
of the vertex process parameter $r$ at a specific point, namely at
$\lambda^{*} = \argmax_{\lambda} \beta(\lambda)$.
\figurename~\ref{fig:power-estimate} shows power as a function of
angle $\theta$, where $\lambda = (cos(\theta), sin(\theta))$.

The main implication that can
be inferred from \figurename~\ref{fig:power-scale} and
\figurename~\ref{fig:power-estimate}
is that inferential performance in the
mathematically tractable first-order and second-order approximation
models does provide guidance for methodological choices applicable to
the exact (realistic but intractable) model. Furthermore, to the
extent that the exact model is realistic, we may tentatively conclude
that approximation model investigations have some bearing on real
data applications.
\begin{figure*}[!t]
  \centering
  \subfloat[$\mathcal{E}_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-scale-size.pdf}
  }
  \hfil
  \subfloat[$\Delta_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-scale-maxd.pdf}
  }
  \hfil
  \subfloat[$\tau_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-scale-scan.pdf}
  }
  \hfil
  \subfloat[$\Psi_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-scale-ntri.pdf}
  }
  \caption{Power $\beta(\theta^{*}_r,r)$ at $\theta_{r}^{*} =
    \argmax_{\theta} \beta(\theta,r)$ as a function of $r$ for the
    four invariants at test size $\alpha = 0.05$. The horizontal lines
    represents first-order approximation $\pm$ three standard
    deviations, and the two curves represent the second approximation
    (green) and exact model (blue). The 10000 Monte Carlo replicates
    yield standard deviations not exceeding $0.005$ for the power
    estimates . The second approximation results match well with the
    exact model results, and both match well for large $r$ with the
    first-order approximation results.  }
  \label{fig:power-scale}
\end{figure*}
\begin{figure*}[!t]
  \centering
  \subfloat[$\mathcal{E}_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-size.pdf}
  }
  \hfil
  \subfloat[$\Delta_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-maxd.pdf}
  }
  \hfil
  \subfloat[$\Psi_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-scan.pdf}
    \label{fig5:subfig_scan}
  }
  \hfil
  \subfloat[$\tau_{\lambda}$]{
    \includegraphics[width=7cm]{graphics/pwr-ntri.pdf}
  }
  \caption{Power $\beta$ as a function of angle $\theta$ for $\lambda
    = (cos(\theta),sin(\theta))$. The plot shows analytic asymptotic
    result (black), and first approximation (red), second
    approximation (green), and exact model (blue) estimates for the
    various invariants via Monte Carlo. The multiple green and blue
    lines correspond to different values of the vertex process rate
    $r$ (here $r \in \{1,2,4,8,16,32,64,128,256,512,1024\}$). 
    The 10000 Monte Carlo
    replicates yields standard deviations not exceeding $0.005$ for
    the power estimates. The four vertical lines in each plot
    correspond to $\theta \in \{0, \pi/2, \pi/4,
    \theta^{*}\}$. Because we are considering a one-sided test and
    because $\pi_{1,k} > \pi_{0,k}$ for both $k = 1$ and $k = 2$, the
    power is maximized in the first quadrant, i.e., $\theta \in (0,
    \pi/2)$. The second approximation results match well
    with the exact model results, and both match well for large $r$
    with the first order approximation and the asymptotic results. The
    optimal $\theta^{*}$ is apparently different for the four different graph
    invariants.  
  }
  \label{fig:power-estimate}
\end{figure*}

Our second example uses the Enron data set which consists of email
messages between 184 executives of the Enron corporation during a time
period from 1998 to 2002. The Enron email messages for most of the
2001 calendar year have been annotated into 32 different topics 
\cite{berry01:_topic_annot_enron_email_data_set} and thus form the
basis of our analysis. We chose to condense these 32 different
topics into two groups. The first group consists of
general energy-related topics and the second group consists of
topics that are specifically related to the Enron corporation. 
From the collection of email messages, we construct a time series of
graphs. Each graph consists of email messages sent during a week in
2001. The vertices of the graphs represent the executives and the edges
represent email communication between the executives. The attributes on
the edges are given by the grouping of the email topics as mentioned
above. 

\begin{figure}[tbp]
  \centering
 \includegraphics[width=8cm]{graphics/enron152.pdf} 
 \caption{Detecting chatter anomaly in the Enron time series of
   graphs. The plot shows the normalized statistics
   $T_{\lambda}^{l}(t)$ as a function of the fusion parameter $\lambda
   = (\cos(\theta), sin(\theta))$ for $l = 20$ and $t = 152$. The
   graph in question ($t = 152$) corresponds to the first week of May
   2001. The dashed lines correspond to the $\theta^{*}$ for the
   different graph invariants ($\theta^{*}_{\mathcal{E}_{\lambda}}
   \approx \theta^{*}_{\Delta_{\lambda}} \approx 0.96$,
   $\theta^{*}_{\Psi_{\lambda}} \approx 1.06$ and
   $\theta^{*}_{\tau_{\lambda}} \approx 0.83$). Attribute fusion
   provides superior detection.}
  \label{fig:enron}
\end{figure}

\begin{figure}[tbp]
  \centering
 \includegraphics[width=8cm]{graphics/Tstat_thetastar.pdf} 
 \caption{The fluctuations of $T_{\lambda}^{l}(t)$ over time for the
   four graph invariants. The value of $\lambda = (\cos(\theta),
   sin(\theta))$ for each graph invariants correspond to the optimal
   fusion parameters for the detection as illustrated in
   \figurename~\ref{fig:enron} and they are
   $\theta^{*}_{\mathcal{E}_{\lambda}} \approx
   \theta^{*}_{\Delta_{\lambda}} \approx 0.96$,
   $\theta^{*}_{\Psi_{\lambda}} \approx 1.06$ and
   $\theta^{*}_{\tau_{\lambda}} \approx 0.83$. The fluctuations
   indicate that there are possible anomalies around the weeks 120,
   132, and 146, which were also reported and analyzed in 
   \cite{priebe05:_scan_statis_enron_graph}.}
\label{fig:enron_time}
\end{figure}
The detection of a chatter anomaly in the Enron data set during the
first week of May 2001 ($t = 152$) was previously
reported in \cite{priebe05:_scan_statis_enron_graph}, but with 
unattributed edges. We chose to investigate the effect
of attribute fusion for that same week in our inference example. 
\figurename~\ref{fig:enron} presents a plot of the normalized
statistic $T_{\lambda}^{l}(t)$ at $t = 152$ with $l = 20$ for the four graph
invariants against the fusion parameter $\lambda = (cos(\theta),
sin(\theta))$. \figurename~\ref{fig:enron_time} presents a plot of the
normalized statistics $T_{\lambda}^{l}(t)$ with $l = 20$ for the four
graph invariants over the time interval corresponding to the
weeks $t = 96$ through $t = 168$. The value of $\lambda$ used
for each graph invariants is the $\lambda$ that maximizes
$T_{\lambda}^{l}(152)$. \figurename~\ref{fig:enron}
indicates that the fusing of attributes leads to better detection
as exemplified by the fact that the maximum for each of the normalized
statistics $T_{\lambda}$ occurs for $\theta \not \in
\{0,\pi/2\}$. Furthermore, the optimal fusion parameters are dependent
on the specific graph invariants.
\section{Conclusions}
We have presented an analysis of change-point detection for time
series of attributed graphs through the use of test statistics which
are based on linear attribute fusion of some graph invariants. 
We derived the limiting distribution of these test statistics under
the assumptions that $n$, the number of vertices, is sufficiently
large and $r$, the process parameter rate, is also sufficiently
large. The limiting distribution for these test statistics are then
used to derive estimates for the power of the tests. 

The simulation experiment in \S \ref{eq:55} indicates that the power estimates
are accurate, even for the moderate value of $n = 100$. Furthermore,
it was also indicated that the optimal linear fusion parameter depends
on the graph invariant considered. In particular, the results depicted
in \figurename~\ref{fig:power-estimate} yield
$\theta^{*}_{\mathcal{E}_{\lambda}} \approx 0.24$,
$\theta^{*}_{\Delta_{\lambda}} \approx 0.32$,
$\theta^{*}_{\Psi_{\lambda}} \approx 0.14$ and
$\theta^{*}_{\tau_{\lambda}} \approx 0.38$. These optimal fusion
parameter differences are statistically significant, and combining
this result with the ``no uniformly most powerful invariant'' result
\cite{pao11:_statis_infer_random_graph}, we conclude that optimal
linear attribute fusion theory requires significant additional
development. Toward this end, the approximation models from
\cite{lee11} promise to be of assistance. 

% The exact generative model for time-series
% of attributed graphs as presented in \S~\ref{sec:power-estimates}
% contains assumptions that are not readily verified. However, as our
% analysis 

Hypothesis testing on time series of attributed graphs has
applications in diverse areas, e.g., social network analysis (wherein
vertices represent individual actors or organizations), connectome
inference (wherein vertices are neurons or brain regions) and text
processing (wherein vertices represent authors or documents). These
and many other applications may benefit from generalizations of our
results and the model in \cite{lee11} to directed, multi, and weighted
graphs, as well as the consideration of inference with errorful edge
attributes through an attribute confusion matrix.

\bibliography{ssp2011}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
